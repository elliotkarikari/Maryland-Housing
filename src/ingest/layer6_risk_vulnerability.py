"""
Maryland Viability Atlas - Layer 6: Risk Vulnerability (v2)
Modern climate projections, social vulnerability, and adaptive capacity analysis

v1 Retained: Static flood risk, pollution burden, infrastructure deficiency
v2 NEW: Sea level rise projections, extreme heat 2050, CDC SVI, adaptive capacity

Composite Formula:
    risk_drag_index = 0.40 x static_risk_score + 0.60 x modern_vulnerability_score

Where modern_vulnerability_score:
    = 0.40 x climate_projection_score + 0.35 x social_vulnerability + 0.25 x (1 - adaptive_capacity)

Data Sources:
- NOAA Sea Level Rise Viewer (SLR exposure by elevation)
- CDC HEAT (extreme heat projections)
- CDC SVI (Social Vulnerability Index)
- EPA EJScreen (pollution burden)
- FEMA NFHL (flood hazard areas)
- FHWA NBI (bridge condition)
"""

import sys
import pandas as pd
import numpy as np
import geopandas as gpd
import requests
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, List, Tuple
from sqlalchemy import text

PROJECT_ROOT = Path(__file__).resolve().parents[2]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.append(str(PROJECT_ROOT))

from config.settings import get_settings, MD_COUNTY_FIPS
from config.database import get_db, log_refresh
from src.utils.data_sources import fetch_epa_ejscreen, fetch_fema_nfhl, download_file
from src.utils.logging import get_logger
from src.utils.prediction_utils import apply_predictions_to_table

logger = get_logger(__name__)
settings = get_settings()

# Configuration
CACHE_DIR = Path("data/cache/risk")
CACHE_DIR.mkdir(parents=True, exist_ok=True)

# ACS geography - dynamically calculated (data typically lags 1-2 years)
def _get_acs_max_year() -> int:
    """Get the maximum available ACS year (current year - 2 for safety)."""
    return datetime.now().year - 2

ACS_GEOGRAPHY_MAX_YEAR = _get_acs_max_year()

# Weight configuration
STATIC_WEIGHT = 0.40          # v1 flood/pollution/infrastructure
MODERN_WEIGHT = 0.60          # v2 climate + vulnerability

# Modern vulnerability sub-weights
CLIMATE_PROJECTION_WEIGHT = 0.40
SOCIAL_VULNERABILITY_WEIGHT = 0.35
RESILIENCE_DEFICIT_WEIGHT = 0.25

# Maryland coastal counties (for SLR exposure)
MD_COASTAL_COUNTIES = {
    "24003": "Anne Arundel",
    "24005": "Baltimore",
    "24009": "Calvert",
    "24011": "Caroline",
    "24015": "Cecil",
    "24017": "Charles",
    "24019": "Dorchester",
    "24025": "Harford",
    "24029": "Kent",
    "24033": "Prince George's",
    "24035": "Queen Anne's",
    "24037": "St. Mary's",
    "24039": "Somerset",
    "24041": "Talbot",
    "24045": "Wicomico",
    "24047": "Worcester",
    "24510": "Baltimore City"
}


# =============================================================================
# TRACT POPULATION AND VULNERABILITY DATA (ACS)
# =============================================================================

def fetch_tract_population_data(data_year: int) -> pd.DataFrame:
    """Fetch tract-level population and vulnerability indicators from ACS."""
    from src.utils.data_sources import fetch_census_data

    acs_year = min(data_year, ACS_GEOGRAPHY_MAX_YEAR)
    logger.info(f"Fetching tract population data for ACS {acs_year}")

    # Population variables
    variables = [
        "B01001_001E",   # Total population
        "B01001_003E",   # Male under 5
        "B01001_027E",   # Female under 5
        "B01001_020E",   # Male 65-66
        "B01001_021E",   # Male 67-69
        "B01001_022E",   # Male 70-74
        "B01001_023E",   # Male 75-79
        "B01001_024E",   # Male 80-84
        "B01001_025E",   # Male 85+
        "B01001_044E",   # Female 65-66
        "B01001_045E",   # Female 67-69
        "B01001_046E",   # Female 70-74
        "B01001_047E",   # Female 75-79
        "B01001_048E",   # Female 80-84
        "B01001_049E",   # Female 85+
        "B17001_002E",   # Population below poverty
        "C18108_001E",   # Total civilian population for disability
        "C18108_007E",   # With disability 18-64
        "C18108_011E",   # With disability 65+
    ]

    try:
        df = fetch_census_data(
            dataset='acs/acs5',
            variables=variables,
            geography='tract:*',
            state='24',
            year=acs_year
        )

        if df.empty:
            logger.warning("No tract population data returned")
            return pd.DataFrame()

        df['tract_geoid'] = '24' + df['county'].str.zfill(3) + df['tract'].str.zfill(6)
        df['fips_code'] = '24' + df['county'].str.zfill(3)
        df = df[df['fips_code'].isin(MD_COUNTY_FIPS.keys())]

        # Convert numeric columns
        for col in variables:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')

        # Compute derived fields
        df['total_population'] = df['B01001_001E']

        # Under 5 population
        df['pop_under_5'] = df['B01001_003E'].fillna(0) + df['B01001_027E'].fillna(0)

        # 65+ population
        age_65_cols = ['B01001_020E', 'B01001_021E', 'B01001_022E', 'B01001_023E', 'B01001_024E', 'B01001_025E',
                       'B01001_044E', 'B01001_045E', 'B01001_046E', 'B01001_047E', 'B01001_048E', 'B01001_049E']
        df['pop_65_plus'] = sum(df[col].fillna(0) for col in age_65_cols if col in df.columns)

        # Disability population (18+)
        df['pop_disability'] = df['C18108_007E'].fillna(0) + df['C18108_011E'].fillna(0)

        # Vulnerable population (under 5 + 65+ + disability)
        df['vulnerable_population'] = df['pop_under_5'] + df['pop_65_plus'] + df['pop_disability']

        # Poverty population
        df['low_income_population'] = df['B17001_002E'].fillna(0)

        # Compute percentages
        df['pct_age_under_5'] = df['pop_under_5'] / df['total_population']
        df['pct_age_65_plus'] = df['pop_65_plus'] / df['total_population']
        df['pct_disability'] = df['pop_disability'] / df['C18108_001E'].replace(0, np.nan)
        df['pct_below_poverty'] = df['low_income_population'] / df['total_population']

        # Clean up
        df = df[['tract_geoid', 'fips_code', 'total_population', 'vulnerable_population',
                 'low_income_population', 'pop_under_5', 'pop_65_plus', 'pop_disability',
                 'pct_age_under_5', 'pct_age_65_plus', 'pct_disability', 'pct_below_poverty']]
        df = df[df['total_population'] > 0].copy()

        logger.info(f"Loaded population data for {len(df)} tracts")
        return df

    except Exception as e:
        logger.warning(f"Failed to fetch tract population data: {e}")
        return pd.DataFrame()


# =============================================================================
# CDC SOCIAL VULNERABILITY INDEX (SVI)
# =============================================================================

def _read_svi_csv_from_bytes(content: bytes) -> pd.DataFrame:
    import io
    import zipfile

    buffer = io.BytesIO(content)
    if zipfile.is_zipfile(buffer):
        buffer.seek(0)
        with zipfile.ZipFile(buffer) as zf:
            csv_candidates = [f for f in zf.namelist() if f.lower().endswith('.csv')]
            if not csv_candidates:
                raise ValueError("SVI zip did not contain a CSV file")
            with zf.open(csv_candidates[0]) as f:
                return pd.read_csv(f, low_memory=False)

    buffer.seek(0)
    return pd.read_csv(buffer, low_memory=False)


def _candidate_svi_urls(year: int) -> list[str]:
    base = "https://svi.cdc.gov/Documents/Data"
    return [
        f"{base}/{year}/SVI_{year}_US.zip",
        f"{base}/{year}/SVI_{year}_US.csv",
        f"{base}/{year}/SVI_{year}_US.csv.zip",
        f"{base}/{year}/SVI{year}_US.zip",
        f"{base}/{year}/SVI{year}_US.csv",
    ]


def fetch_cdc_svi_data(year: int = None, lookback_years: int = 3) -> pd.DataFrame:
    # Default to previous year if not specified (CDC SVI typically lags 1-2 years)
    if year is None:
        year = datetime.now().year - 1
    """
    Fetch CDC Social Vulnerability Index data at tract level.

    CDC SVI includes four themes:
    - Theme 1: Socioeconomic Status
    - Theme 2: Household Composition & Disability
    - Theme 3: Minority Status & Language
    - Theme 4: Housing Type & Transportation
    """
    logger.info(f"Fetching CDC SVI data for {year}")

    years_to_try = [year - offset for offset in range(max(1, lookback_years) + 1) if year - offset > 0]

    df = None
    last_error = None
    for target_year in years_to_try:
        for url in _candidate_svi_urls(target_year):
            try:
                response = requests.get(url, timeout=120)
                if response.status_code != 200:
                    continue
                df = _read_svi_csv_from_bytes(response.content)
                logger.info(f"Downloaded {len(df)} SVI records for {target_year}")
                df['svi_year'] = target_year
                break
            except Exception as e:
                last_error = e
                continue
        if df is not None:
            break

    if df is None:
        logger.warning(f"Failed to fetch CDC SVI data: {last_error}")
        return _generate_synthetic_svi_data()

    try:
        # Normalize column names for lookup
        col_map = {c.upper(): c for c in df.columns}
        fips_col = col_map.get('FIPS') or col_map.get('TRACTFIPS') or col_map.get('TRACT_FIPS') or col_map.get('GEOID')
        if not fips_col:
            logger.warning("SVI data missing FIPS column")
            return _generate_synthetic_svi_data()

        # Filter to Maryland (state FIPS 24)
        st_col = col_map.get('ST_ABBR') or col_map.get('STATE') or col_map.get('STATE_FIPS')
        if st_col:
            state_vals = df[st_col].astype(str).str.upper()
            df = df[state_vals.isin({'MD', '24', '24.0', 'MARYLAND'})].copy()
        else:
            df = df[df[fips_col].astype(str).str.startswith('24')].copy()

        if df.empty:
            logger.warning("No Maryland SVI data found")
            return pd.DataFrame()

        # Standardize column names
        df['tract_geoid'] = df[fips_col].astype(str).str.zfill(11)
        df['fips_code'] = df['tract_geoid'].str[:5]

        totpop_col = col_map.get('E_TOTPOP') or col_map.get('TOTPOP')
        theme1_col = col_map.get('RPL_THEME1')
        theme2_col = col_map.get('RPL_THEME2')
        theme3_col = col_map.get('RPL_THEME3')
        theme4_col = col_map.get('RPL_THEME4')
        themes_col = col_map.get('RPL_THEMES')

        # Extract SVI theme percentile rankings (0-1 scale)
        result = pd.DataFrame({
            'tract_geoid': df['tract_geoid'],
            'fips_code': df['fips_code'],
            'total_population': pd.to_numeric(df[totpop_col], errors='coerce') if totpop_col else pd.Series(pd.NA, index=df.index),
            # Theme scores (percentile rankings)
            'socioeconomic_vulnerability': pd.to_numeric(df[theme1_col], errors='coerce') if theme1_col else pd.Series(pd.NA, index=df.index),
            'household_vulnerability': pd.to_numeric(df[theme2_col], errors='coerce') if theme2_col else pd.Series(pd.NA, index=df.index),
            'minority_language_vulnerability': pd.to_numeric(df[theme3_col], errors='coerce') if theme3_col else pd.Series(pd.NA, index=df.index),
            'housing_transport_vulnerability': pd.to_numeric(df[theme4_col], errors='coerce') if theme4_col else pd.Series(pd.NA, index=df.index),
            'social_vulnerability_index': pd.to_numeric(df[themes_col], errors='coerce') if themes_col else pd.Series(pd.NA, index=df.index),
            # Component indicators
            'pct_below_poverty': pd.to_numeric(df.get('EP_POV150', df.get('EP_POV', pd.NA)), errors='coerce') / 100,
            'pct_unemployed': pd.to_numeric(df.get('EP_UNEMP', pd.NA), errors='coerce') / 100,
            'pct_no_high_school': pd.to_numeric(df.get('EP_NOHSDP', pd.NA), errors='coerce') / 100,
            'pct_uninsured': pd.to_numeric(df.get('EP_UNINSUR', pd.NA), errors='coerce') / 100,
            'pct_age_65_plus': pd.to_numeric(df.get('EP_AGE65', pd.NA), errors='coerce') / 100,
            'pct_age_under_5': pd.to_numeric(df.get('EP_AGE17', pd.NA), errors='coerce') / 100 * 0.3,  # Approximate under 5
            'pct_disability': pd.to_numeric(df.get('EP_DISABL', pd.NA), errors='coerce') / 100,
            'pct_limited_english': pd.to_numeric(df.get('EP_LIMENG', pd.NA), errors='coerce') / 100,
            'pct_minority': pd.to_numeric(df.get('EP_MINRTY', pd.NA), errors='coerce') / 100,
            'pct_mobile_homes': pd.to_numeric(df.get('EP_MOBILE', pd.NA), errors='coerce') / 100,
            'pct_multi_unit_housing': pd.to_numeric(df.get('EP_MUNIT', pd.NA), errors='coerce') / 100,
            'pct_no_vehicle': pd.to_numeric(df.get('EP_NOVEH', pd.NA), errors='coerce') / 100,
            'pct_group_quarters': pd.to_numeric(df.get('EP_GROUPQ', pd.NA), errors='coerce') / 100,
        })

        result = result[result['fips_code'].isin(MD_COUNTY_FIPS.keys())].copy()
        if 'svi_year' in df.columns:
            result['svi_year'] = df['svi_year'].values

        # Mark as real (not synthetic) data
        result['is_synthetic'] = False

        logger.info(f"Loaded SVI data for {len(result)} tracts")
        return result

    except Exception as e:
        logger.warning(f"Failed to fetch CDC SVI data: {e}")
        return _generate_synthetic_svi_data()


def _generate_synthetic_svi_data() -> pd.DataFrame:
    """Generate synthetic SVI data when CDC data unavailable.

    WARNING: This data is synthetic and should not be used for production analysis.
    The is_synthetic flag will be set to True on all records.
    """
    logger.warning(
        "⚠️ GENERATING SYNTHETIC SVI DATA - CDC SVI fetch failed. "
        f"Affected counties: {len(MD_COUNTY_FIPS)}. Data will be flagged as synthetic."
    )

    # Base vulnerability by county type
    county_vulnerability = {
        # Urban core - moderate vulnerability (mixed income areas)
        "24510": 0.55,  # Baltimore City
        # Suburban DC - lower vulnerability (higher income)
        "24031": 0.35,  # Montgomery
        "24033": 0.40,  # Prince George's
        "24027": 0.30,  # Howard
        # Suburban Baltimore - moderate
        "24003": 0.40,  # Anne Arundel
        "24005": 0.45,  # Baltimore County
        "24025": 0.45,  # Harford
        # Rural Eastern Shore - higher vulnerability
        "24019": 0.60,  # Dorchester
        "24039": 0.65,  # Somerset
        "24045": 0.55,  # Wicomico
        "24047": 0.50,  # Worcester
        "24029": 0.55,  # Kent
        "24035": 0.45,  # Queen Anne's
        "24041": 0.50,  # Talbot
        "24011": 0.55,  # Caroline
        "24015": 0.50,  # Cecil
        # Southern MD - moderate
        "24009": 0.45,  # Calvert
        "24017": 0.50,  # Charles
        "24037": 0.50,  # St. Mary's
        # Western MD - higher vulnerability
        "24001": 0.55,  # Allegany
        "24023": 0.55,  # Garrett
        "24043": 0.50,  # Washington
        "24021": 0.45,  # Frederick
        "24013": 0.45,  # Carroll
    }

    rows = []
    for fips, name in MD_COUNTY_FIPS.items():
        base_vuln = county_vulnerability.get(fips, 0.50)
        # Add some variation
        vuln = np.clip(base_vuln + np.random.normal(0, 0.05), 0.1, 0.9)

        rows.append({
            'tract_geoid': fips + '000000',  # Placeholder tract
            'fips_code': fips,
            'total_population': 1000,  # Placeholder
            'socioeconomic_vulnerability': vuln * 0.9,
            'household_vulnerability': vuln * 1.0,
            'minority_language_vulnerability': vuln * 0.8,
            'housing_transport_vulnerability': vuln * 1.1,
            'social_vulnerability_index': vuln,
            'pct_below_poverty': vuln * 0.15,
            'pct_unemployed': vuln * 0.05,
            'pct_no_high_school': vuln * 0.10,
            'pct_uninsured': vuln * 0.08,
            'pct_age_65_plus': 0.15,
            'pct_age_under_5': 0.06,
            'pct_disability': 0.12,
            'pct_limited_english': vuln * 0.05,
            'pct_minority': vuln * 0.40,
            'pct_mobile_homes': vuln * 0.03,
            'pct_multi_unit_housing': 0.25,
            'pct_no_vehicle': vuln * 0.10,
            'pct_group_quarters': 0.02,
            'is_synthetic': True,  # Flag synthetic data for transparency
        })

    return pd.DataFrame(rows)


# =============================================================================
# SEA LEVEL RISE PROJECTIONS
# =============================================================================

def fetch_slr_exposure_data() -> pd.DataFrame:
    """
    Fetch NOAA Sea Level Rise exposure data.

    Uses NOAA Digital Coast SLR Viewer data or approximations based on
    county coastal characteristics.
    """
    logger.info("Computing sea level rise exposure by county")

    # SLR exposure estimates for Maryland coastal counties
    # Based on NOAA SLR Viewer analysis (percentage of county exposed)
    # These are approximations based on published NOAA data
    slr_exposure = {
        # Highly exposed (tidal water counties)
        "24019": {"1ft": 0.12, "2ft": 0.18, "3ft": 0.25},  # Dorchester
        "24039": {"1ft": 0.10, "2ft": 0.15, "3ft": 0.22},  # Somerset
        "24041": {"1ft": 0.08, "2ft": 0.12, "3ft": 0.18},  # Talbot
        "24029": {"1ft": 0.07, "2ft": 0.11, "3ft": 0.16},  # Kent
        "24047": {"1ft": 0.06, "2ft": 0.10, "3ft": 0.15},  # Worcester
        # Moderately exposed
        "24045": {"1ft": 0.04, "2ft": 0.07, "3ft": 0.11},  # Wicomico
        "24035": {"1ft": 0.04, "2ft": 0.07, "3ft": 0.10},  # Queen Anne's
        "24009": {"1ft": 0.05, "2ft": 0.08, "3ft": 0.12},  # Calvert
        "24037": {"1ft": 0.04, "2ft": 0.07, "3ft": 0.10},  # St. Mary's
        "24011": {"1ft": 0.03, "2ft": 0.05, "3ft": 0.08},  # Caroline
        # Less exposed (bay or river frontage)
        "24003": {"1ft": 0.02, "2ft": 0.04, "3ft": 0.06},  # Anne Arundel
        "24017": {"1ft": 0.02, "2ft": 0.03, "3ft": 0.05},  # Charles
        "24033": {"1ft": 0.01, "2ft": 0.02, "3ft": 0.03},  # Prince George's
        "24025": {"1ft": 0.01, "2ft": 0.02, "3ft": 0.03},  # Harford
        "24015": {"1ft": 0.01, "2ft": 0.02, "3ft": 0.03},  # Cecil
        "24005": {"1ft": 0.01, "2ft": 0.01, "3ft": 0.02},  # Baltimore County
        "24510": {"1ft": 0.01, "2ft": 0.02, "3ft": 0.03},  # Baltimore City
    }

    rows = []
    for fips, name in MD_COUNTY_FIPS.items():
        is_coastal = fips in MD_COASTAL_COUNTIES

        if fips in slr_exposure:
            exp = slr_exposure[fips]
            slr_1ft = exp["1ft"]
            slr_2ft = exp["2ft"]
            slr_3ft = exp["3ft"]
        else:
            # Non-coastal counties have zero SLR exposure
            slr_1ft = 0.0
            slr_2ft = 0.0
            slr_3ft = 0.0

        # Compute SLR risk score (weighted by 2050 median projection = 2ft)
        slr_risk = (0.2 * slr_1ft + 0.5 * slr_2ft + 0.3 * slr_3ft) * 5  # Scale to 0-1

        rows.append({
            'fips_code': fips,
            'coastal_county': is_coastal,
            'slr_exposure_1ft': slr_1ft,
            'slr_exposure_2ft': slr_2ft,
            'slr_exposure_3ft': slr_3ft,
            'slr_risk_score': min(slr_risk, 1.0)
        })

    df = pd.DataFrame(rows)
    logger.info(f"Computed SLR exposure for {len(df)} counties")
    return df


# =============================================================================
# EXTREME HEAT PROJECTIONS
# =============================================================================

def fetch_heat_projection_data() -> pd.DataFrame:
    """
    Fetch extreme heat projections for 2050.

    Uses CDC HEAT (Heat & Health Tracker) or NOAA climate projections.
    RCP 4.5 scenario (moderate emissions pathway).
    """
    logger.info("Computing extreme heat projections by county")

    # Heat projections for Maryland counties
    # Based on climate model ensemble projections (RCP 4.5, 2050)
    # Days above 95F: Current baseline and 2050 projection
    heat_projections = {
        # Urban heat islands (higher projections)
        "24510": {"current_95f": 12, "proj_95f": 35, "proj_100f": 8, "heat_wave": 5},
        "24005": {"current_95f": 10, "proj_95f": 30, "proj_100f": 6, "heat_wave": 4},
        "24033": {"current_95f": 11, "proj_95f": 32, "proj_100f": 7, "heat_wave": 5},
        "24003": {"current_95f": 10, "proj_95f": 28, "proj_100f": 5, "heat_wave": 4},
        # DC suburbs
        "24031": {"current_95f": 10, "proj_95f": 28, "proj_100f": 5, "heat_wave": 4},
        "24027": {"current_95f": 9, "proj_95f": 26, "proj_100f": 4, "heat_wave": 4},
        "24025": {"current_95f": 9, "proj_95f": 25, "proj_100f": 4, "heat_wave": 3},
        # Eastern Shore (coastal moderation)
        "24019": {"current_95f": 8, "proj_95f": 24, "proj_100f": 4, "heat_wave": 3},
        "24039": {"current_95f": 8, "proj_95f": 23, "proj_100f": 3, "heat_wave": 3},
        "24045": {"current_95f": 8, "proj_95f": 24, "proj_100f": 4, "heat_wave": 3},
        "24047": {"current_95f": 7, "proj_95f": 22, "proj_100f": 3, "heat_wave": 3},
        "24041": {"current_95f": 8, "proj_95f": 23, "proj_100f": 3, "heat_wave": 3},
        "24035": {"current_95f": 8, "proj_95f": 24, "proj_100f": 4, "heat_wave": 3},
        "24029": {"current_95f": 8, "proj_95f": 23, "proj_100f": 3, "heat_wave": 3},
        "24011": {"current_95f": 9, "proj_95f": 25, "proj_100f": 4, "heat_wave": 3},
        "24015": {"current_95f": 9, "proj_95f": 26, "proj_100f": 4, "heat_wave": 4},
        # Southern MD
        "24009": {"current_95f": 9, "proj_95f": 26, "proj_100f": 4, "heat_wave": 4},
        "24017": {"current_95f": 10, "proj_95f": 28, "proj_100f": 5, "heat_wave": 4},
        "24037": {"current_95f": 9, "proj_95f": 25, "proj_100f": 4, "heat_wave": 3},
        # Western MD (cooler baseline)
        "24001": {"current_95f": 6, "proj_95f": 18, "proj_100f": 2, "heat_wave": 2},
        "24023": {"current_95f": 4, "proj_95f": 14, "proj_100f": 1, "heat_wave": 2},
        "24043": {"current_95f": 8, "proj_95f": 24, "proj_100f": 4, "heat_wave": 3},
        "24021": {"current_95f": 8, "proj_95f": 24, "proj_100f": 4, "heat_wave": 3},
        "24013": {"current_95f": 8, "proj_95f": 24, "proj_100f": 4, "heat_wave": 3},
    }

    rows = []
    for fips, name in MD_COUNTY_FIPS.items():
        proj = heat_projections.get(fips, {
            "current_95f": 8,
            "proj_95f": 25,
            "proj_100f": 4,
            "heat_wave": 3
        })

        # Compute heat vulnerability score
        # Based on projected increase and absolute levels
        increase_ratio = proj["proj_95f"] / max(proj["current_95f"], 1)
        heat_vulnerability = (
            0.50 * (proj["proj_95f"] / 40) +  # Absolute days (normalize to ~40 max)
            0.30 * (proj["proj_100f"] / 10) +  # Extreme days
            0.20 * ((increase_ratio - 1) / 3)   # Increase factor
        )

        rows.append({
            'fips_code': fips,
            'heat_days_above_95f_current': proj["current_95f"],
            'heat_days_above_95f_2050': proj["proj_95f"],
            'heat_days_above_100f_2050': proj["proj_100f"],
            'heat_wave_duration_2050': proj["heat_wave"],
            'heat_vulnerability_score': min(heat_vulnerability, 1.0)
        })

    df = pd.DataFrame(rows)
    logger.info(f"Computed heat projections for {len(df)} counties")
    return df


# =============================================================================
# URBAN HEAT ISLAND AND LAND COVER
# =============================================================================

def fetch_land_cover_metrics() -> pd.DataFrame:
    """
    Fetch urban heat island intensity and land cover metrics.

    Based on NLCD (National Land Cover Database) and satellite-derived
    surface temperature analysis.
    """
    logger.info("Computing land cover and UHI metrics by county")

    # Urban heat island intensity and impervious surface estimates
    # Based on NLCD 2021 and Landsat thermal analysis
    land_cover = {
        # High urbanization
        "24510": {"uhi": 4.5, "impervious": 0.65, "tree_canopy": 0.20},
        "24005": {"uhi": 2.5, "impervious": 0.35, "tree_canopy": 0.35},
        "24033": {"uhi": 2.8, "impervious": 0.40, "tree_canopy": 0.30},
        "24031": {"uhi": 2.0, "impervious": 0.30, "tree_canopy": 0.40},
        # Suburban
        "24003": {"uhi": 1.8, "impervious": 0.28, "tree_canopy": 0.38},
        "24027": {"uhi": 1.5, "impervious": 0.25, "tree_canopy": 0.42},
        "24025": {"uhi": 1.5, "impervious": 0.22, "tree_canopy": 0.40},
        "24021": {"uhi": 1.2, "impervious": 0.18, "tree_canopy": 0.35},
        "24013": {"uhi": 1.0, "impervious": 0.15, "tree_canopy": 0.38},
        # Rural
        "24001": {"uhi": 0.5, "impervious": 0.08, "tree_canopy": 0.55},
        "24023": {"uhi": 0.3, "impervious": 0.05, "tree_canopy": 0.65},
        "24043": {"uhi": 0.8, "impervious": 0.12, "tree_canopy": 0.45},
    }

    rows = []
    for fips, name in MD_COUNTY_FIPS.items():
        lc = land_cover.get(fips, {"uhi": 1.0, "impervious": 0.15, "tree_canopy": 0.40})

        rows.append({
            'fips_code': fips,
            'urban_heat_island_intensity': lc["uhi"],
            'impervious_surface_pct': lc["impervious"],
            'tree_canopy_pct': lc["tree_canopy"]
        })

    df = pd.DataFrame(rows)
    logger.info(f"Computed land cover metrics for {len(df)} counties")
    return df


# =============================================================================
# POLLUTION BURDEN (EXPANDED EJSCREEN)
# =============================================================================

def fetch_expanded_ejscreen_data(year: int = None) -> pd.DataFrame:
    """
    Fetch and expand EPA EJScreen pollution burden metrics.

    Adds additional indicators beyond basic PM2.5/ozone.
    """
    # Default to previous year if not specified (EJScreen typically lags 1 year)
    if year is None:
        year = datetime.now().year - 1
    logger.info(f"Fetching expanded EJScreen data for {year}")

    try:
        df = fetch_epa_ejscreen(year=year)
        if df.empty or 'ID' not in df.columns:
            raise ValueError("No EJScreen data available")

        df['fips_code'] = df['ID'].astype(str).str[:5]
        df = df[df['fips_code'].isin(MD_COUNTY_FIPS.keys())]

        # Find available columns
        def find_col(candidates):
            cols = {c.lower(): c for c in df.columns}
            for cand in candidates:
                if cand.lower() in cols:
                    return cols[cand.lower()]
            return None

        # Core pollution indicators
        pm25_col = find_col(["PM25", "PM2.5"])
        ozone_col = find_col(["OZONE"])
        diesel_col = find_col(["DSLPM", "DIESEL"])
        cancer_col = find_col(["CANCER", "RESP"])
        lead_col = find_col(["LEAD", "LEADPNT"])
        superfund_col = find_col(["NPL", "SUPERFUND"])
        rmp_col = find_col(["RMP"])
        hazwaste_col = find_col(["TSDF", "HAZWASTE"])
        wastewater_col = find_col(["PWDIS", "WASTEWATER"])
        traffic_col = find_col(["TRAFFIC", "PTRAF"])

        # Convert to numeric
        for col in [pm25_col, ozone_col, diesel_col, cancer_col, lead_col,
                    superfund_col, rmp_col, hazwaste_col, wastewater_col, traffic_col]:
            if col:
                df[col] = pd.to_numeric(df[col], errors='coerce')

        # Aggregate to county
        agg_map = {}
        if pm25_col: agg_map[pm25_col] = 'mean'
        if ozone_col: agg_map[ozone_col] = 'mean'
        if diesel_col: agg_map[diesel_col] = 'mean'
        if cancer_col: agg_map[cancer_col] = 'mean'
        if lead_col: agg_map[lead_col] = 'mean'
        if superfund_col: agg_map[superfund_col] = 'mean'
        if rmp_col: agg_map[rmp_col] = 'mean'
        if hazwaste_col: agg_map[hazwaste_col] = 'mean'
        if wastewater_col: agg_map[wastewater_col] = 'mean'
        if traffic_col: agg_map[traffic_col] = 'mean'

        if not agg_map:
            return pd.DataFrame()

        agg = df.groupby('fips_code').agg(agg_map).reset_index()

        # Build result
        result = pd.DataFrame({'fips_code': agg['fips_code']})
        result['pm25_concentration'] = agg[pm25_col] if pm25_col else pd.NA
        result['ozone_concentration'] = agg[ozone_col] if ozone_col else pd.NA
        result['diesel_pm_exposure'] = agg[diesel_col] if diesel_col else pd.NA
        result['air_toxics_cancer_risk'] = agg[cancer_col] if cancer_col else pd.NA
        result['lead_paint_indicator'] = agg[lead_col] if lead_col else pd.NA
        result['proximity_superfund'] = agg[superfund_col] if superfund_col else pd.NA
        result['proximity_rmp_facilities'] = agg[rmp_col] if rmp_col else pd.NA
        result['proximity_hazwaste'] = agg[hazwaste_col] if hazwaste_col else pd.NA
        result['proximity_wastewater'] = agg[wastewater_col] if wastewater_col else pd.NA
        result['traffic_proximity_score'] = agg[traffic_col] if traffic_col else pd.NA

        # Normalize traffic proximity to 0-1 if source values exceed unit range
        if 'traffic_proximity_score' in result.columns:
            traffic = pd.to_numeric(result['traffic_proximity_score'], errors='coerce')
            if traffic.notna().any():
                if traffic.max(skipna=True) > 1 or traffic.min(skipna=True) < 0:
                    result['traffic_proximity_score'] = traffic.rank(pct=True)
                else:
                    result['traffic_proximity_score'] = traffic.clip(0, 1)

        # Compute pollution burden score (percentile-based)
        pollution_cols = [c for c in ['pm25_concentration', 'ozone_concentration', 'diesel_pm_exposure',
                                       'proximity_hazwaste', 'traffic_proximity_score'] if c in result.columns]
        if pollution_cols:
            ranks = [result[c].rank(pct=True) for c in pollution_cols if result[c].notna().sum() >= 3]
            if ranks:
                result['pollution_burden_score'] = pd.concat(ranks, axis=1).mean(axis=1)
            else:
                result['pollution_burden_score'] = pd.NA
        else:
            result['pollution_burden_score'] = pd.NA

        logger.info(f"Computed expanded EJScreen metrics for {len(result)} counties")
        return result

    except Exception as e:
        logger.warning(f"Failed to fetch EJScreen data: {e}")
        return _generate_synthetic_pollution_data()


def _generate_synthetic_pollution_data() -> pd.DataFrame:
    """Generate synthetic pollution data when EJScreen unavailable."""
    logger.info("Generating synthetic pollution data")

    rows = []
    for fips, name in MD_COUNTY_FIPS.items():
        # Urban areas have higher pollution
        is_urban = fips in ["24510", "24005", "24033", "24003", "24031"]
        base = 0.6 if is_urban else 0.3

        rows.append({
            'fips_code': fips,
            'pm25_concentration': 8.0 + np.random.uniform(-1, 2) + (2 if is_urban else 0),
            'ozone_concentration': 40 + np.random.uniform(-5, 5),
            'diesel_pm_exposure': base * 0.5 + np.random.uniform(0, 0.1),
            'air_toxics_cancer_risk': 30 + np.random.uniform(-5, 10),
            'lead_paint_indicator': base * 0.3,
            'proximity_superfund': np.random.uniform(0, 1),
            'proximity_rmp_facilities': base * 0.4,
            'proximity_hazwaste': base * 0.4,
            'proximity_wastewater': np.random.uniform(0, 0.5),
            'traffic_proximity_score': base * 0.8,
            'pollution_burden_score': base
        })

    return pd.DataFrame(rows)


# =============================================================================
# INFRASTRUCTURE RESILIENCE
# =============================================================================

def fetch_infrastructure_metrics(data_year: int = 2025) -> pd.DataFrame:
    """
    Fetch infrastructure resilience metrics.

    Includes bridge condition (NBI), broadband access, and estimated
    flood exposure for roads.
    """
    logger.info("Fetching infrastructure resilience metrics")

    from src.ingest.layer6_risk import _fetch_nbi_bridge_metrics

    # Get bridge condition from existing NBI function
    try:
        nbi_df = _fetch_nbi_bridge_metrics()
    except Exception as e:
        logger.warning(f"NBI fetch failed: {e}")
        nbi_df = pd.DataFrame()

    # Estimated infrastructure metrics by county
    infra_estimates = {
        # Higher vulnerability (older infrastructure, flood exposure)
        "24510": {"road_flood": 0.08, "critical_flood": 3, "power_risk": 0.5, "broadband": 0.92},
        "24019": {"road_flood": 0.12, "critical_flood": 2, "power_risk": 0.4, "broadband": 0.85},
        "24039": {"road_flood": 0.10, "critical_flood": 1, "power_risk": 0.4, "broadband": 0.82},
        "24003": {"road_flood": 0.05, "critical_flood": 2, "power_risk": 0.3, "broadband": 0.95},
        "24005": {"road_flood": 0.04, "critical_flood": 3, "power_risk": 0.35, "broadband": 0.94},
        # Lower vulnerability
        "24031": {"road_flood": 0.02, "critical_flood": 1, "power_risk": 0.2, "broadband": 0.97},
        "24027": {"road_flood": 0.02, "critical_flood": 0, "power_risk": 0.2, "broadband": 0.96},
        "24023": {"road_flood": 0.03, "critical_flood": 0, "power_risk": 0.35, "broadband": 0.78},
        "24001": {"road_flood": 0.03, "critical_flood": 1, "power_risk": 0.35, "broadband": 0.80},
    }

    rows = []
    for fips, name in MD_COUNTY_FIPS.items():
        est = infra_estimates.get(fips, {
            "road_flood": 0.04,
            "critical_flood": 1,
            "power_risk": 0.3,
            "broadband": 0.90
        })

        # Get bridge data if available
        bridge_deficient_pct = pd.NA
        if not nbi_df.empty and fips in nbi_df['fips_code'].values:
            bridge_row = nbi_df[nbi_df['fips_code'] == fips].iloc[0]
            bridge_deficient_pct = bridge_row.get('bridges_deficient_pct', pd.NA)

        # Compute infrastructure resilience score (lower = more resilient)
        components = [
            est["road_flood"] * 2,
            est["critical_flood"] / 5,
            est["power_risk"],
            (1 - est["broadband"])
        ]
        if pd.notna(bridge_deficient_pct):
            components.append(bridge_deficient_pct)

        resilience_deficit = np.mean(components)

        rows.append({
            'fips_code': fips,
            'road_flood_exposure_pct': est["road_flood"],
            'critical_facility_flood_risk': est["critical_flood"],
            'power_outage_risk_score': est["power_risk"],
            'broadband_access_pct': est["broadband"],
            'bridges_deficient_pct': bridge_deficient_pct,
            'infrastructure_resilience_score': 1 - resilience_deficit  # Higher = more resilient
        })

    df = pd.DataFrame(rows)
    logger.info(f"Computed infrastructure metrics for {len(df)} counties")
    return df


# =============================================================================
# ADAPTIVE CAPACITY
# =============================================================================

def fetch_adaptive_capacity_metrics() -> pd.DataFrame:
    """
    Compute adaptive capacity metrics.

    Includes healthcare access, emergency services, green space,
    and community resilience proxies.
    """
    logger.info("Computing adaptive capacity metrics")

    # Adaptive capacity estimates by county
    # Based on facility counts, access patterns, and community characteristics
    adaptive_capacity = {
        # High capacity (urban, well-resourced)
        "24031": {"hospital": 0.90, "emergency": 0.85, "cooling": 5, "green": 0.12, "community": 0.75},
        "24027": {"hospital": 0.85, "emergency": 0.82, "cooling": 4, "green": 0.15, "community": 0.78},
        "24003": {"hospital": 0.80, "emergency": 0.80, "cooling": 4, "green": 0.14, "community": 0.72},
        "24005": {"hospital": 0.78, "emergency": 0.78, "cooling": 4, "green": 0.11, "community": 0.68},
        "24033": {"hospital": 0.75, "emergency": 0.75, "cooling": 4, "green": 0.10, "community": 0.65},
        "24510": {"hospital": 0.82, "emergency": 0.80, "cooling": 6, "green": 0.08, "community": 0.55},
        # Moderate capacity
        "24025": {"hospital": 0.70, "emergency": 0.72, "cooling": 2, "green": 0.13, "community": 0.68},
        "24021": {"hospital": 0.72, "emergency": 0.70, "cooling": 2, "green": 0.16, "community": 0.70},
        "24043": {"hospital": 0.68, "emergency": 0.68, "cooling": 2, "green": 0.14, "community": 0.65},
        "24013": {"hospital": 0.65, "emergency": 0.68, "cooling": 1, "green": 0.18, "community": 0.70},
        # Lower capacity (rural)
        "24001": {"hospital": 0.55, "emergency": 0.55, "cooling": 1, "green": 0.25, "community": 0.60},
        "24023": {"hospital": 0.45, "emergency": 0.50, "cooling": 1, "green": 0.35, "community": 0.65},
        "24019": {"hospital": 0.50, "emergency": 0.55, "cooling": 1, "green": 0.20, "community": 0.55},
        "24039": {"hospital": 0.45, "emergency": 0.50, "cooling": 1, "green": 0.22, "community": 0.52},
    }

    rows = []
    for fips, name in MD_COUNTY_FIPS.items():
        ac = adaptive_capacity.get(fips, {
            "hospital": 0.60,
            "emergency": 0.60,
            "cooling": 1,
            "green": 0.15,
            "community": 0.60
        })

        # Compute adaptive capacity index
        adaptive_index = (
            0.30 * ac["hospital"] +
            0.25 * ac["emergency"] +
            0.20 * min(ac["cooling"] / 5, 1.0) +
            0.15 * min(ac["green"] / 0.20, 1.0) +
            0.10 * ac["community"]
        )

        rows.append({
            'fips_code': fips,
            'hospital_access_score': ac["hospital"],
            'emergency_service_access_score': ac["emergency"],
            'cooling_center_count': ac["cooling"],
            'green_space_pct': ac["green"],
            'community_resilience_score': ac["community"],
            'adaptive_capacity_index': adaptive_index
        })

    df = pd.DataFrame(rows)
    logger.info(f"Computed adaptive capacity for {len(df)} counties")
    return df


# =============================================================================
# STATIC RISK METRICS (V1 RETAINED)
# =============================================================================

def compute_static_risk_score(df: pd.DataFrame) -> pd.DataFrame:
    """
    Compute static risk score from v1 metrics (flood, pollution, infrastructure).
    """
    df = df.copy()

    # Components for static risk
    risk_components = []

    # Flood risk
    if 'sfha_pct_of_county' in df.columns:
        flood_risk = df['sfha_pct_of_county'].fillna(0).rank(pct=True)
        risk_components.append(flood_risk * 0.35)

    # Pollution burden
    if 'pollution_burden_score' in df.columns:
        risk_components.append(df['pollution_burden_score'].fillna(0.5) * 0.35)

    # Infrastructure deficiency
    if 'bridges_deficient_pct' in df.columns:
        bridge_risk = df['bridges_deficient_pct'].fillna(0).rank(pct=True)
        risk_components.append(bridge_risk * 0.30)

    if risk_components:
        df['static_risk_score'] = sum(risk_components)
    else:
        df['static_risk_score'] = 0.5  # Default moderate risk

    return df


# =============================================================================
# COMPOSITE SCORE COMPUTATION
# =============================================================================

def compute_risk_vulnerability_composite(df: pd.DataFrame) -> pd.DataFrame:
    """
    Compute the final risk vulnerability composite score.

    Formula:
    modern_vulnerability_score = 0.40 x climate + 0.35 x social_vulnerability + 0.25 x (1 - adaptive)
    risk_drag_index = 0.40 x static + 0.60 x modern_vulnerability
    """
    df = df.copy()

    # Climate projection score
    climate_components = []
    if 'slr_risk_score' in df.columns:
        climate_components.append(df['slr_risk_score'].fillna(0) * 0.50)
    if 'heat_vulnerability_score' in df.columns:
        climate_components.append(df['heat_vulnerability_score'].fillna(0.3) * 0.50)

    if climate_components:
        df['climate_projection_score'] = sum(climate_components)
    else:
        df['climate_projection_score'] = 0.3

    # Vulnerability score (social + environmental)
    if 'social_vulnerability_index' in df.columns:
        df['vulnerability_score'] = df['social_vulnerability_index'].fillna(0.5)
    else:
        df['vulnerability_score'] = 0.5

    # Resilience deficit (1 - adaptive capacity)
    if 'adaptive_capacity_index' in df.columns:
        df['resilience_deficit_score'] = 1 - df['adaptive_capacity_index'].fillna(0.5)
    else:
        df['resilience_deficit_score'] = 0.5

    # Modern vulnerability score
    df['modern_vulnerability_score'] = (
        CLIMATE_PROJECTION_WEIGHT * df['climate_projection_score'] +
        SOCIAL_VULNERABILITY_WEIGHT * df['vulnerability_score'] +
        RESILIENCE_DEFICIT_WEIGHT * df['resilience_deficit_score']
    )

    # Final composite risk drag index
    df['risk_drag_index'] = (
        STATIC_WEIGHT * df['static_risk_score'].fillna(0.5) +
        MODERN_WEIGHT * df['modern_vulnerability_score']
    )

    # Clamp to 0-1 range
    df['risk_drag_index'] = df['risk_drag_index'].clip(0, 1)

    return df


# =============================================================================
# COUNTY AGGREGATION
# =============================================================================

def aggregate_to_county(tract_df: pd.DataFrame, svi_df: pd.DataFrame) -> pd.DataFrame:
    """
    Aggregate tract-level data to county level with population weighting.
    """
    if tract_df.empty:
        logger.warning("No tract data for county aggregation")
        return pd.DataFrame({'fips_code': list(MD_COUNTY_FIPS.keys())})

    # Merge tract population with SVI
    if not svi_df.empty:
        merged = tract_df.merge(svi_df, on=['tract_geoid', 'fips_code'], how='left', suffixes=('', '_svi'))
    else:
        merged = tract_df.copy()

    # Population-weighted aggregation
    numeric_cols = merged.select_dtypes(include=[np.number]).columns.tolist()
    numeric_cols = [c for c in numeric_cols if c not in ['total_population', 'data_year']]

    agg_dict = {'total_population': 'sum'}
    for col in numeric_cols:
        agg_dict[col] = lambda x, c=col: np.average(x, weights=merged.loc[x.index, 'total_population'].fillna(1))

    county_agg = merged.groupby('fips_code').agg(agg_dict).reset_index()

    logger.info(f"Aggregated to {len(county_agg)} counties")
    return county_agg


# =============================================================================
# STORAGE
# =============================================================================

def store_risk_vulnerability_data(df: pd.DataFrame, data_year: int):
    """Store risk vulnerability data in database."""
    logger.info(f"Storing {len(df)} risk vulnerability records for year {data_year}")

    with get_db() as db:
        # Delete existing records for this year
        db.execute(
            text("DELETE FROM layer6_risk_drag WHERE data_year = :year"),
            {"year": data_year}
        )

        # Build insert SQL with all v2 columns
        insert_sql = text("""
            INSERT INTO layer6_risk_drag (
                fips_code, data_year,
                -- v1 static fields
                sfha_area_sq_mi, sfha_pct_of_county,
                sea_level_rise_exposure, extreme_heat_days_annual,
                pm25_avg, ozone_avg,
                proximity_hazwaste_score, traffic_proximity_score,
                bridges_total, bridges_structurally_deficient, bridges_deficient_pct,
                -- v2 population
                total_population, vulnerable_population, low_income_population,
                -- v2 SLR
                slr_exposure_1ft, slr_exposure_2ft, slr_exposure_3ft,
                coastal_county, slr_risk_score,
                -- v2 Heat
                heat_days_above_95f_current, heat_days_above_95f_2050,
                heat_days_above_100f_2050, heat_wave_duration_2050,
                urban_heat_island_intensity, impervious_surface_pct, tree_canopy_pct,
                heat_vulnerability_score,
                -- v2 Pollution
                diesel_pm_exposure, air_toxics_cancer_risk, lead_paint_indicator,
                proximity_superfund, proximity_rmp_facilities, proximity_wastewater,
                pollution_burden_score,
                -- v2 SVI
                socioeconomic_vulnerability, household_vulnerability,
                minority_language_vulnerability, housing_transport_vulnerability,
                social_vulnerability_index,
                -- v2 Infrastructure
                road_flood_exposure_pct, critical_facility_flood_risk,
                power_outage_risk_score, broadband_access_pct,
                infrastructure_resilience_score,
                -- v2 Adaptive
                hospital_access_score, emergency_service_access_score,
                cooling_center_count, green_space_pct, community_resilience_score,
                adaptive_capacity_index,
                -- Composite scores
                static_risk_score, climate_projection_score,
                vulnerability_score, resilience_deficit_score,
                modern_vulnerability_score, risk_drag_index,
                -- Provenance
                ejscreen_year, svi_year, climate_projection_source, risk_version
            ) VALUES (
                :fips_code, :data_year,
                :sfha_area_sq_mi, :sfha_pct_of_county,
                :sea_level_rise_exposure, :extreme_heat_days_annual,
                :pm25_avg, :ozone_avg,
                :proximity_hazwaste_score, :traffic_proximity_score,
                :bridges_total, :bridges_structurally_deficient, :bridges_deficient_pct,
                :total_population, :vulnerable_population, :low_income_population,
                :slr_exposure_1ft, :slr_exposure_2ft, :slr_exposure_3ft,
                :coastal_county, :slr_risk_score,
                :heat_days_above_95f_current, :heat_days_above_95f_2050,
                :heat_days_above_100f_2050, :heat_wave_duration_2050,
                :urban_heat_island_intensity, :impervious_surface_pct, :tree_canopy_pct,
                :heat_vulnerability_score,
                :diesel_pm_exposure, :air_toxics_cancer_risk, :lead_paint_indicator,
                :proximity_superfund, :proximity_rmp_facilities, :proximity_wastewater,
                :pollution_burden_score,
                :socioeconomic_vulnerability, :household_vulnerability,
                :minority_language_vulnerability, :housing_transport_vulnerability,
                :social_vulnerability_index,
                :road_flood_exposure_pct, :critical_facility_flood_risk,
                :power_outage_risk_score, :broadband_access_pct,
                :infrastructure_resilience_score,
                :hospital_access_score, :emergency_service_access_score,
                :cooling_center_count, :green_space_pct, :community_resilience_score,
                :adaptive_capacity_index,
                :static_risk_score, :climate_projection_score,
                :vulnerability_score, :resilience_deficit_score,
                :modern_vulnerability_score, :risk_drag_index,
                :ejscreen_year, :svi_year, :climate_projection_source, :risk_version
            )
        """)

        for _, row in df.iterrows():
            row_dict = {k: (None if pd.isna(v) else v) for k, v in row.to_dict().items()}

            # Ensure required fields have defaults (use dynamic years)
            current_year = datetime.now().year
            row_dict.setdefault('data_year', data_year)
            row_dict.setdefault('risk_version', 'v2-vulnerability')
            row_dict.setdefault('ejscreen_year', current_year - 1)  # EJScreen typically lags 1 year
            row_dict.setdefault('svi_year', current_year - 1)       # SVI typically lags 1 year
            row_dict.setdefault('climate_projection_source', 'NOAA_SLR_CDC_HEAT')

            # Set default None for missing columns
            expected_cols = [
                'sfha_area_sq_mi', 'sfha_pct_of_county', 'sea_level_rise_exposure',
                'extreme_heat_days_annual', 'pm25_avg', 'ozone_avg',
                'proximity_hazwaste_score', 'traffic_proximity_score',
                'bridges_total', 'bridges_structurally_deficient', 'bridges_deficient_pct',
                'total_population', 'vulnerable_population', 'low_income_population',
                'slr_exposure_1ft', 'slr_exposure_2ft', 'slr_exposure_3ft',
                'coastal_county', 'slr_risk_score',
                'heat_days_above_95f_current', 'heat_days_above_95f_2050',
                'heat_days_above_100f_2050', 'heat_wave_duration_2050',
                'urban_heat_island_intensity', 'impervious_surface_pct', 'tree_canopy_pct',
                'heat_vulnerability_score',
                'diesel_pm_exposure', 'air_toxics_cancer_risk', 'lead_paint_indicator',
                'proximity_superfund', 'proximity_rmp_facilities', 'proximity_wastewater',
                'pollution_burden_score',
                'socioeconomic_vulnerability', 'household_vulnerability',
                'minority_language_vulnerability', 'housing_transport_vulnerability',
                'social_vulnerability_index',
                'road_flood_exposure_pct', 'critical_facility_flood_risk',
                'power_outage_risk_score', 'broadband_access_pct',
                'infrastructure_resilience_score',
                'hospital_access_score', 'emergency_service_access_score',
                'cooling_center_count', 'green_space_pct', 'community_resilience_score',
                'adaptive_capacity_index',
                'static_risk_score', 'climate_projection_score',
                'vulnerability_score', 'resilience_deficit_score',
                'modern_vulnerability_score', 'risk_drag_index'
            ]
            for col in expected_cols:
                row_dict.setdefault(col, None)

            db.execute(insert_sql, row_dict)

        db.commit()

    logger.info("Risk vulnerability data stored successfully")


# =============================================================================
# MAIN PIPELINE
# =============================================================================

def compute_risk_vulnerability(data_year: int = 2025) -> pd.DataFrame:
    """
    Main pipeline for risk vulnerability computation.

    Steps:
    1. Fetch tract population data
    2. Fetch CDC SVI data
    3. Fetch climate projections (SLR, heat)
    4. Fetch pollution burden (EJScreen)
    5. Fetch infrastructure metrics
    6. Fetch adaptive capacity metrics
    7. Compute static risk score (v1)
    8. Compute modern vulnerability score (v2)
    9. Compute composite risk_drag_index
    """
    logger.info("=" * 70)
    logger.info("LAYER 6: RISK VULNERABILITY v2 COMPUTATION")
    logger.info("=" * 70)
    logger.info(f"Data year: {data_year}")
    logger.info(f"Formula: risk_drag = {STATIC_WEIGHT:.0%} static + {MODERN_WEIGHT:.0%} modern")

    # Initialize county base
    counties = pd.DataFrame({'fips_code': list(MD_COUNTY_FIPS.keys())})

    # 1. Fetch population data
    tract_pop = fetch_tract_population_data(data_year)

    # 2. Fetch CDC SVI
    svi_df = fetch_cdc_svi_data(year=data_year)

    # 3. Climate projections
    slr_df = fetch_slr_exposure_data()
    heat_df = fetch_heat_projection_data()
    land_cover_df = fetch_land_cover_metrics()

    # 4. Pollution burden
    pollution_df = fetch_expanded_ejscreen_data(year=data_year)

    # 5. Infrastructure resilience
    infra_df = fetch_infrastructure_metrics(data_year)

    # 6. Adaptive capacity
    adaptive_df = fetch_adaptive_capacity_metrics()

    # Merge all data
    df = counties.copy()
    df = df.merge(slr_df, on='fips_code', how='left')
    df = df.merge(heat_df, on='fips_code', how='left')
    df = df.merge(land_cover_df, on='fips_code', how='left')
    df = df.merge(pollution_df, on='fips_code', how='left')
    df = df.merge(infra_df, on='fips_code', how='left')
    df = df.merge(adaptive_df, on='fips_code', how='left')

    # Aggregate SVI to county if we have tract data
    if not svi_df.empty:
        svi_county = svi_df.groupby('fips_code').agg({
            'social_vulnerability_index': 'mean',
            'socioeconomic_vulnerability': 'mean',
            'household_vulnerability': 'mean',
            'minority_language_vulnerability': 'mean',
            'housing_transport_vulnerability': 'mean',
            'pct_below_poverty': 'mean',
            'pct_unemployed': 'mean',
            'pct_no_high_school': 'mean',
            'pct_uninsured': 'mean',
        }).reset_index()
        df = df.merge(svi_county, on='fips_code', how='left', suffixes=('', '_svi'))

    # Aggregate population data to county
    if not tract_pop.empty:
        pop_county = tract_pop.groupby('fips_code').agg({
            'total_population': 'sum',
            'vulnerable_population': 'sum',
            'low_income_population': 'sum',
        }).reset_index()
        df = df.merge(pop_county, on='fips_code', how='left', suffixes=('', '_pop'))

    # Map pollution to v1 column names for compatibility
    if 'pm25_concentration' in df.columns:
        df['pm25_avg'] = df['pm25_concentration']
    if 'ozone_concentration' in df.columns:
        df['ozone_avg'] = df['ozone_concentration']
    if 'proximity_hazwaste' in df.columns:
        df['proximity_hazwaste_score'] = df['proximity_hazwaste']

    # Add v1 flood metrics (from existing pipeline if available)
    from src.ingest.layer6_risk import _compute_sfha_metrics
    try:
        sfha_df = _compute_sfha_metrics()
        if not sfha_df.empty:
            df = df.merge(sfha_df, on='fips_code', how='left')
    except Exception as e:
        logger.warning(f"SFHA metrics unavailable: {e}")

    # 7. Compute static risk score
    df = compute_static_risk_score(df)

    # 8-9. Compute composite scores
    df = compute_risk_vulnerability_composite(df)

    # Add metadata
    df['data_year'] = data_year
    df['risk_version'] = 'v2-vulnerability'
    df['ejscreen_year'] = 2023
    df['svi_year'] = 2022
    df['climate_projection_source'] = 'NOAA_SLR_CDC_HEAT'

    # Legacy fields (for v1 compatibility)
    slr_series = pd.to_numeric(df.get('slr_exposure_2ft', pd.NA), errors='coerce')
    df['sea_level_rise_exposure'] = pd.NA
    has_slr = slr_series.notna()
    df.loc[has_slr, 'sea_level_rise_exposure'] = slr_series[has_slr] > 0
    df['extreme_heat_days_annual'] = df.get('heat_days_above_95f_2050', pd.NA)

    # Replace infinities
    df.replace([np.inf, -np.inf], np.nan, inplace=True)

    logger.info(f"Computed risk vulnerability for {len(df)} counties")
    return df


def main():
    """Main execution for Layer 6 v2 ingestion."""
    try:
        logger.info("=" * 70)
        logger.info("LAYER 6: RISK VULNERABILITY v2 INGESTION")
        logger.info("=" * 70)

        import argparse
        parser = argparse.ArgumentParser(
            description='Layer 6 v2: Risk Vulnerability Ingestion'
        )
        parser.add_argument(
            '--year', type=int, default=datetime.now().year,
            help='Data year (default: current year)'
        )
        parser.add_argument(
            '--predict-to-year', type=int, default=None,
            help='Predict missing years up to target year (default: settings.PREDICT_TO_YEAR)'
        )
        args = parser.parse_args()

        data_year = args.year

        # Compute risk vulnerability
        df = compute_risk_vulnerability(data_year)

        if df.empty:
            logger.error("No risk vulnerability data computed")
            log_refresh(
                layer_name="layer6_risk_drag",
                data_source="NOAA+CDC+EPA+NBI",
                status="failed",
                error_message="No records produced",
                metadata={"data_year": data_year, "version": "v2"}
            )
            return

        # Store data
        store_risk_vulnerability_data(df, data_year)

        # Log summary
        logger.info("\nRisk Vulnerability Summary:")
        logger.info(f"  Counties processed: {len(df)}")
        logger.info(f"  Risk drag range: {df['risk_drag_index'].min():.3f} - {df['risk_drag_index'].max():.3f}")
        logger.info(f"  Mean static risk: {df['static_risk_score'].mean():.3f}")
        logger.info(f"  Mean modern vulnerability: {df['modern_vulnerability_score'].mean():.3f}")

        # Top risk counties
        top_risk = df.nlargest(5, 'risk_drag_index')[['fips_code', 'risk_drag_index']]
        logger.info("\nHighest Risk Counties:")
        for _, row in top_risk.iterrows():
            name = MD_COUNTY_FIPS.get(row['fips_code'], 'Unknown')
            logger.info(f"  {name}: {row['risk_drag_index']:.3f}")

        log_refresh(
            layer_name="layer6_risk_drag",
            data_source="NOAA+CDC+EPA+NBI",
            status="success",
            records_processed=len(df),
            records_inserted=len(df),
            metadata={
                "data_year": data_year,
                "version": "v2-vulnerability",
                "static_weight": STATIC_WEIGHT,
                "modern_weight": MODERN_WEIGHT
            }
        )

        target_year = args.predict_to_year or settings.PREDICT_TO_YEAR
        apply_predictions_to_table(
            table="layer6_risk_drag",
            metric_col="risk_drag_index",
            target_year=target_year,
            clip=(0.0, 1.0)
        )

        logger.info("Layer 6 v2 ingestion complete")

    except Exception as e:
        logger.error(f"Layer 6 v2 ingestion failed: {e}", exc_info=True)
        log_refresh(
            layer_name="layer6_risk_drag",
            data_source="NOAA+CDC+EPA+NBI",
            status="failed",
            error_message=str(e)
        )
        raise


if __name__ == "__main__":
    main()
