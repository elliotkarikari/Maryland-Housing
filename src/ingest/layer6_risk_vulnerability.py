"""
Maryland Viability Atlas - Layer 6: Risk Vulnerability (v2)
Modern climate projections, social vulnerability, and adaptive capacity analysis

v1 Retained: Static flood risk, pollution burden, infrastructure deficiency
v2 NEW: Sea level rise projections, extreme heat 2050, CDC SVI, adaptive capacity

Composite Formula:
    risk_drag_index = 0.40 x static_risk_score + 0.60 x modern_vulnerability_score

Where modern_vulnerability_score:
    = 0.40 x climate_projection_score + 0.35 x social_vulnerability + 0.25 x (1 - adaptive_capacity)

Data Sources:
- NOAA Sea Level Rise Viewer (SLR exposure by elevation)
- CDC HEAT (extreme heat projections)
- CDC SVI (Social Vulnerability Index)
- EPA EJScreen (pollution burden)
- FEMA NFHL (flood hazard areas)
- FHWA NBI (bridge condition)
"""

import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import geopandas as gpd
import numpy as np
import pandas as pd
import requests
from sqlalchemy import text

PROJECT_ROOT = Path(__file__).resolve().parents[2]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.append(str(PROJECT_ROOT))

from config.database import get_db, log_refresh
from config.database import table_name as db_table_name
from config.settings import MD_COUNTY_FIPS, get_settings
from src.utils.data_sources import download_file, fetch_epa_ejscreen, fetch_fema_nfhl
from src.utils.db_bulk import execute_batch
from src.utils.logging import get_logger
from src.utils.prediction_utils import apply_predictions_to_table
from src.utils.year_policy import acs_geography_year, pipeline_default_year

logger = get_logger(__name__)
settings = get_settings()
L6_COUNTY_TABLE = db_table_name("layer6_risk_drag")

# Configuration
CACHE_DIR = Path("data/cache/risk")
CACHE_DIR.mkdir(parents=True, exist_ok=True)


# ACS geography - dynamically calculated (data typically lags 1-2 years)
def _get_acs_max_year() -> int:
    """Get the configured maximum ACS geography year."""
    return settings.ACS_GEOGRAPHY_MAX_YEAR


ACS_GEOGRAPHY_MAX_YEAR = _get_acs_max_year()

# Weight configuration
STATIC_WEIGHT = 0.40  # v1 flood/pollution/infrastructure
MODERN_WEIGHT = 0.60  # v2 climate + vulnerability

# Modern vulnerability sub-weights
CLIMATE_PROJECTION_WEIGHT = 0.40
SOCIAL_VULNERABILITY_WEIGHT = 0.35
RESILIENCE_DEFICIT_WEIGHT = 0.25

# Maryland coastal counties (for SLR exposure)
MD_COASTAL_COUNTIES = {
    "24003": "Anne Arundel",
    "24005": "Baltimore",
    "24009": "Calvert",
    "24011": "Caroline",
    "24015": "Cecil",
    "24017": "Charles",
    "24019": "Dorchester",
    "24025": "Harford",
    "24029": "Kent",
    "24033": "Prince George's",
    "24035": "Queen Anne's",
    "24037": "St. Mary's",
    "24039": "Somerset",
    "24041": "Talbot",
    "24045": "Wicomico",
    "24047": "Worcester",
    "24510": "Baltimore City",
}


# =============================================================================
# TRACT POPULATION AND VULNERABILITY DATA (ACS)
# =============================================================================


def fetch_tract_population_data(data_year: int) -> pd.DataFrame:
    """Fetch tract-level population and vulnerability indicators from ACS."""
    from src.utils.data_sources import fetch_census_data

    acs_year = acs_geography_year(data_year)
    logger.info(f"Fetching tract population data for ACS {acs_year}")

    # Population variables
    variables = [
        "B01001_001E",  # Total population
        "B01001_003E",  # Male under 5
        "B01001_027E",  # Female under 5
        "B01001_020E",  # Male 65-66
        "B01001_021E",  # Male 67-69
        "B01001_022E",  # Male 70-74
        "B01001_023E",  # Male 75-79
        "B01001_024E",  # Male 80-84
        "B01001_025E",  # Male 85+
        "B01001_044E",  # Female 65-66
        "B01001_045E",  # Female 67-69
        "B01001_046E",  # Female 70-74
        "B01001_047E",  # Female 75-79
        "B01001_048E",  # Female 80-84
        "B01001_049E",  # Female 85+
        "B17001_002E",  # Population below poverty
        "C18108_001E",  # Total civilian population for disability
        "C18108_007E",  # With disability 18-64
        "C18108_011E",  # With disability 65+
    ]

    try:
        df = fetch_census_data(
            dataset="acs/acs5", variables=variables, geography="tract:*", state="24", year=acs_year
        )

        if df.empty:
            logger.warning("No tract population data returned")
            return pd.DataFrame()

        df["tract_geoid"] = "24" + df["county"].str.zfill(3) + df["tract"].str.zfill(6)
        df["fips_code"] = "24" + df["county"].str.zfill(3)
        df = df[df["fips_code"].isin(MD_COUNTY_FIPS.keys())]

        # Convert numeric columns
        for col in variables:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors="coerce")

        # Compute derived fields
        df["total_population"] = df["B01001_001E"]

        # Under 5 population
        df["pop_under_5"] = df["B01001_003E"].fillna(0) + df["B01001_027E"].fillna(0)

        # 65+ population
        age_65_cols = [
            "B01001_020E",
            "B01001_021E",
            "B01001_022E",
            "B01001_023E",
            "B01001_024E",
            "B01001_025E",
            "B01001_044E",
            "B01001_045E",
            "B01001_046E",
            "B01001_047E",
            "B01001_048E",
            "B01001_049E",
        ]
        df["pop_65_plus"] = sum(df[col].fillna(0) for col in age_65_cols if col in df.columns)

        # Disability population (18+)
        df["pop_disability"] = df["C18108_007E"].fillna(0) + df["C18108_011E"].fillna(0)

        # Vulnerable population (under 5 + 65+ + disability)
        df["vulnerable_population"] = df["pop_under_5"] + df["pop_65_plus"] + df["pop_disability"]

        # Poverty population
        df["low_income_population"] = df["B17001_002E"].fillna(0)

        # Compute percentages
        df["pct_age_under_5"] = df["pop_under_5"] / df["total_population"]
        df["pct_age_65_plus"] = df["pop_65_plus"] / df["total_population"]
        df["pct_disability"] = df["pop_disability"] / df["C18108_001E"].replace(0, np.nan)
        df["pct_below_poverty"] = df["low_income_population"] / df["total_population"]

        # Clean up
        df = df[
            [
                "tract_geoid",
                "fips_code",
                "total_population",
                "vulnerable_population",
                "low_income_population",
                "pop_under_5",
                "pop_65_plus",
                "pop_disability",
                "pct_age_under_5",
                "pct_age_65_plus",
                "pct_disability",
                "pct_below_poverty",
            ]
        ]
        df = df[df["total_population"] > 0].copy()

        logger.info(f"Loaded population data for {len(df)} tracts")
        return df

    except Exception as e:
        logger.warning(f"Failed to fetch tract population data: {e}")
        return pd.DataFrame()


# =============================================================================
# CDC SOCIAL VULNERABILITY INDEX (SVI)
# =============================================================================


def _read_svi_csv_from_bytes(content: bytes) -> pd.DataFrame:
    import io
    import zipfile

    buffer = io.BytesIO(content)
    if zipfile.is_zipfile(buffer):
        buffer.seek(0)
        with zipfile.ZipFile(buffer) as zf:
            csv_candidates = [f for f in zf.namelist() if f.lower().endswith(".csv")]
            if not csv_candidates:
                raise ValueError("SVI zip did not contain a CSV file")
            with zf.open(csv_candidates[0]) as f:
                return pd.read_csv(f, low_memory=False)

    buffer.seek(0)
    return pd.read_csv(buffer, low_memory=False)


def _candidate_svi_urls(year: int) -> list[str]:
    base = "https://svi.cdc.gov/Documents/Data"
    atsdr = "https://www.atsdr.cdc.gov/placeandhealth/svi/data/svi_data"
    return [
        f"{atsdr}/{year}/SVI{year}_US_tract.csv",
        f"{atsdr}/{year}/SVI{year}_US_tract.csv.zip",
        f"{base}/{year}/SVI_{year}_US.zip",
        f"{base}/{year}/SVI_{year}_US.csv",
        f"{base}/{year}/SVI_{year}_US.csv.zip",
        f"{base}/{year}/SVI{year}_US.zip",
        f"{base}/{year}/SVI{year}_US.csv",
    ]


def _fetch_svi_arcgis() -> pd.DataFrame:
    """
    Fetch SVI 2022 tract data from CDC/ATSDR ArcGIS service with pagination.
    """
    base_url = (
        "https://onemap.cdc.gov/onemapservices/rest/services/SVI/"
        "CDC_ATSDR_Social_Vulnerability_Index_2022_USA/FeatureServer/2/query"
    )
    params: Dict[str, Any] = {
        "where": "ST_ABBR='MD'",
        "outFields": "*",
        "f": "json",
        "resultRecordCount": 2000,
    }
    record_count = int(params["resultRecordCount"])
    all_rows = []
    offset = 0
    for _ in range(200):
        params["resultOffset"] = offset
        resp = requests.get(base_url, params=params, timeout=120)
        if resp.status_code != 200:
            break
        payload = resp.json()
        features = payload.get("features", [])
        if not features:
            break
        for feat in features:
            attrs = feat.get("attributes")
            if attrs:
                all_rows.append(attrs)
        if len(features) < record_count:
            break
        offset += record_count

    if not all_rows:
        return pd.DataFrame()

    df = pd.DataFrame(all_rows)
    df["source_url"] = base_url
    df["fetch_date"] = datetime.utcnow().date().isoformat()
    df["is_real"] = True
    return df


def fetch_cdc_svi_data(year: int = None, lookback_years: int = 3) -> pd.DataFrame:
    # Default to previous year if not specified (CDC SVI typically lags 1-2 years)
    if year is None:
        year = datetime.now().year - 1
    """
    Fetch CDC Social Vulnerability Index data at tract level.

    CDC SVI includes four themes:
    - Theme 1: Socioeconomic Status
    - Theme 2: Household Composition & Disability
    - Theme 3: Minority Status & Language
    - Theme 4: Housing Type & Transportation
    """
    logger.info(f"Fetching CDC SVI data for {year}")

    years_to_try = [
        year - offset for offset in range(max(1, lookback_years) + 1) if year - offset > 0
    ]

    df = None
    last_error = None
    last_url = None
    for target_year in years_to_try:
        for url in _candidate_svi_urls(target_year):
            try:
                response = requests.get(url, timeout=120)
                if response.status_code != 200:
                    continue
                df = _read_svi_csv_from_bytes(response.content)
                last_url = url
                logger.info(f"Downloaded {len(df)} SVI records for {target_year}")
                df["svi_year"] = target_year
                break
            except Exception as e:
                last_error = e
                continue
        if df is not None:
            break

    if df is None:
        logger.warning(f"CSV fetch failed for CDC SVI data: {last_error}. Trying ArcGIS service.")
        df = _fetch_svi_arcgis()
        last_url = (
            df.get("source_url").iloc[0]
            if not df.empty and "source_url" in df.columns
            else last_url
        )
        if df is None or df.empty:
            return pd.DataFrame()

    try:
        # Normalize column names for lookup
        col_map = {c.upper(): c for c in df.columns}
        fips_col = (
            col_map.get("FIPS")
            or col_map.get("TRACTFIPS")
            or col_map.get("TRACT_FIPS")
            or col_map.get("GEOID")
        )
        if not fips_col:
            logger.warning("SVI data missing FIPS column")
            return pd.DataFrame()

        # Filter to Maryland (state FIPS 24)
        st_col = col_map.get("ST_ABBR") or col_map.get("STATE") or col_map.get("STATE_FIPS")
        if st_col:
            state_vals = df[st_col].astype(str).str.upper()
            df = df[state_vals.isin({"MD", "24", "24.0", "MARYLAND"})].copy()
        else:
            df = df[df[fips_col].astype(str).str.startswith("24")].copy()

        if df.empty:
            logger.warning("No Maryland SVI data found")
            return pd.DataFrame()

        # Standardize column names
        df["tract_geoid"] = df[fips_col].astype(str).str.zfill(11)
        df["fips_code"] = df["tract_geoid"].str[:5]

        totpop_col = col_map.get("E_TOTPOP") or col_map.get("TOTPOP")
        theme1_col = col_map.get("RPL_THEME1")
        theme2_col = col_map.get("RPL_THEME2")
        theme3_col = col_map.get("RPL_THEME3")
        theme4_col = col_map.get("RPL_THEME4")
        themes_col = col_map.get("RPL_THEMES")

        # Extract SVI theme percentile rankings (0-1 scale)
        result = pd.DataFrame(
            {
                "tract_geoid": df["tract_geoid"],
                "fips_code": df["fips_code"],
                "total_population": (
                    pd.to_numeric(df[totpop_col], errors="coerce")
                    if totpop_col
                    else pd.Series(pd.NA, index=df.index)
                ),
                # Theme scores (percentile rankings)
                "socioeconomic_vulnerability": (
                    pd.to_numeric(df[theme1_col], errors="coerce")
                    if theme1_col
                    else pd.Series(pd.NA, index=df.index)
                ),
                "household_vulnerability": (
                    pd.to_numeric(df[theme2_col], errors="coerce")
                    if theme2_col
                    else pd.Series(pd.NA, index=df.index)
                ),
                "minority_language_vulnerability": (
                    pd.to_numeric(df[theme3_col], errors="coerce")
                    if theme3_col
                    else pd.Series(pd.NA, index=df.index)
                ),
                "housing_transport_vulnerability": (
                    pd.to_numeric(df[theme4_col], errors="coerce")
                    if theme4_col
                    else pd.Series(pd.NA, index=df.index)
                ),
                "social_vulnerability_index": (
                    pd.to_numeric(df[themes_col], errors="coerce")
                    if themes_col
                    else pd.Series(pd.NA, index=df.index)
                ),
                # Component indicators
                "pct_below_poverty": pd.to_numeric(
                    df.get("EP_POV150", df.get("EP_POV", pd.NA)), errors="coerce"
                )
                / 100,
                "pct_unemployed": pd.to_numeric(df.get("EP_UNEMP", pd.NA), errors="coerce") / 100,
                "pct_no_high_school": pd.to_numeric(df.get("EP_NOHSDP", pd.NA), errors="coerce")
                / 100,
                "pct_uninsured": pd.to_numeric(df.get("EP_UNINSUR", pd.NA), errors="coerce") / 100,
                "pct_age_65_plus": pd.to_numeric(df.get("EP_AGE65", pd.NA), errors="coerce") / 100,
                "pct_age_under_5": pd.to_numeric(df.get("EP_AGE17", pd.NA), errors="coerce")
                / 100
                * 0.3,  # Approximate under 5
                "pct_disability": pd.to_numeric(df.get("EP_DISABL", pd.NA), errors="coerce") / 100,
                "pct_limited_english": pd.to_numeric(df.get("EP_LIMENG", pd.NA), errors="coerce")
                / 100,
                "pct_minority": pd.to_numeric(df.get("EP_MINRTY", pd.NA), errors="coerce") / 100,
                "pct_mobile_homes": pd.to_numeric(df.get("EP_MOBILE", pd.NA), errors="coerce")
                / 100,
                "pct_multi_unit_housing": pd.to_numeric(df.get("EP_MUNIT", pd.NA), errors="coerce")
                / 100,
                "pct_no_vehicle": pd.to_numeric(df.get("EP_NOVEH", pd.NA), errors="coerce") / 100,
                "pct_group_quarters": pd.to_numeric(df.get("EP_GROUPQ", pd.NA), errors="coerce")
                / 100,
            }
        )

        result = result[result["fips_code"].isin(MD_COUNTY_FIPS.keys())].copy()
        if "svi_year" in df.columns:
            result["svi_year"] = df["svi_year"].values

        result["source_url"] = last_url
        result["fetch_date"] = datetime.utcnow().date().isoformat()
        result["is_real"] = True
        result["is_synthetic"] = False

        logger.info(f"Loaded SVI data for {len(result)} tracts")
        return result

    except Exception as e:
        logger.warning(f"Failed to fetch CDC SVI data: {e}")
        return pd.DataFrame()


def _generate_synthetic_svi_data() -> pd.DataFrame:
    """Generate synthetic SVI data when CDC data unavailable.

    WARNING: This data is synthetic and should not be used for production analysis.
    The is_synthetic flag will be set to True on all records.
    """
    logger.warning(
        "⚠️ GENERATING SYNTHETIC SVI DATA - CDC SVI fetch failed. "
        f"Affected counties: {len(MD_COUNTY_FIPS)}. Data will be flagged as synthetic."
    )

    # Base vulnerability by county type
    county_vulnerability = {
        # Urban core - moderate vulnerability (mixed income areas)
        "24510": 0.55,  # Baltimore City
        # Suburban DC - lower vulnerability (higher income)
        "24031": 0.35,  # Montgomery
        "24033": 0.40,  # Prince George's
        "24027": 0.30,  # Howard
        # Suburban Baltimore - moderate
        "24003": 0.40,  # Anne Arundel
        "24005": 0.45,  # Baltimore County
        "24025": 0.45,  # Harford
        # Rural Eastern Shore - higher vulnerability
        "24019": 0.60,  # Dorchester
        "24039": 0.65,  # Somerset
        "24045": 0.55,  # Wicomico
        "24047": 0.50,  # Worcester
        "24029": 0.55,  # Kent
        "24035": 0.45,  # Queen Anne's
        "24041": 0.50,  # Talbot
        "24011": 0.55,  # Caroline
        "24015": 0.50,  # Cecil
        # Southern MD - moderate
        "24009": 0.45,  # Calvert
        "24017": 0.50,  # Charles
        "24037": 0.50,  # St. Mary's
        # Western MD - higher vulnerability
        "24001": 0.55,  # Allegany
        "24023": 0.55,  # Garrett
        "24043": 0.50,  # Washington
        "24021": 0.45,  # Frederick
        "24013": 0.45,  # Carroll
    }

    rows = []
    for fips, name in MD_COUNTY_FIPS.items():
        base_vuln = county_vulnerability.get(fips, 0.50)
        # Add some variation
        vuln = np.clip(base_vuln + np.random.normal(0, 0.05), 0.1, 0.9)

        rows.append(
            {
                "tract_geoid": fips + "000000",  # Placeholder tract
                "fips_code": fips,
                "total_population": 1000,  # Placeholder
                "socioeconomic_vulnerability": vuln * 0.9,
                "household_vulnerability": vuln * 1.0,
                "minority_language_vulnerability": vuln * 0.8,
                "housing_transport_vulnerability": vuln * 1.1,
                "social_vulnerability_index": vuln,
                "pct_below_poverty": vuln * 0.15,
                "pct_unemployed": vuln * 0.05,
                "pct_no_high_school": vuln * 0.10,
                "pct_uninsured": vuln * 0.08,
                "pct_age_65_plus": 0.15,
                "pct_age_under_5": 0.06,
                "pct_disability": 0.12,
                "pct_limited_english": vuln * 0.05,
                "pct_minority": vuln * 0.40,
                "pct_mobile_homes": vuln * 0.03,
                "pct_multi_unit_housing": 0.25,
                "pct_no_vehicle": vuln * 0.10,
                "pct_group_quarters": 0.02,
                "is_synthetic": True,  # Flag synthetic data for transparency
            }
        )

    return pd.DataFrame(rows)


# =============================================================================
# SEA LEVEL RISE PROJECTIONS
# =============================================================================


def fetch_slr_exposure_data() -> pd.DataFrame:
    """
    Fetch NOAA Sea Level Rise exposure data.

    Uses NOAA Digital Coast SLR Viewer data when a CSV URL is configured.
    """
    logger.info("Fetching sea level rise exposure by county")

    if not settings.NOAA_SLR_DATA_URL and not settings.NOAA_SLR_VECTOR_URLS:
        logger.warning("NOAA SLR URLs not configured; skipping SLR exposure ingestion")
        return pd.DataFrame()

    if settings.NOAA_SLR_DATA_URL:
        cache_path = CACHE_DIR / "noaa_slr_exposure.csv"
        if not cache_path.exists():
            ok = download_file(settings.NOAA_SLR_DATA_URL, str(cache_path))
            if not ok:
                logger.warning("Failed to download NOAA SLR data")
                return pd.DataFrame()

        df = pd.read_csv(cache_path, dtype=str, low_memory=False)
        df.columns = [c.strip().lower() for c in df.columns]
    else:
        df = None

    def _pick_col(cands):
        for cand in cands:
            if cand in df.columns:
                return cand
        return None

    if df is not None:
        fips_col = _pick_col(["fips", "fips_code", "county_fips", "geoid"])
        slr1_col = _pick_col(["slr_1ft", "slr_exposure_1ft", "exposure_1ft", "pct_1ft"])
        slr2_col = _pick_col(["slr_2ft", "slr_exposure_2ft", "exposure_2ft", "pct_2ft"])
        slr3_col = _pick_col(["slr_3ft", "slr_exposure_3ft", "exposure_3ft", "pct_3ft"])

        if not fips_col:
            logger.warning("NOAA SLR file missing county FIPS column")
            return pd.DataFrame()

        df["fips_code"] = df[fips_col].astype(str).str.zfill(5)
        df = df[df["fips_code"].isin(MD_COUNTY_FIPS.keys())].copy()
        if df.empty:
            return pd.DataFrame()

        for col in [slr1_col, slr2_col, slr3_col]:
            if col:
                df[col] = pd.to_numeric(df[col], errors="coerce")

        result = pd.DataFrame(
            {
                "fips_code": df["fips_code"],
                "coastal_county": df["fips_code"].isin(MD_COASTAL_COUNTIES),
                "slr_exposure_1ft": df[slr1_col] if slr1_col else pd.NA,
                "slr_exposure_2ft": df[slr2_col] if slr2_col else pd.NA,
                "slr_exposure_3ft": df[slr3_col] if slr3_col else pd.NA,
            }
        )
        source_url = settings.NOAA_SLR_DATA_URL
    else:
        try:
            import zipfile
            from pathlib import Path

            import geopandas as gpd
            import pygris
        except Exception as e:
            logger.warning(f"Geospatial deps missing for NOAA SLR vector processing: {e}")
            return pd.DataFrame()

        urls = [u.strip() for u in (settings.NOAA_SLR_VECTOR_URLS or "").split(";") if u.strip()]
        if not urls:
            return pd.DataFrame()

        gdfs = []
        for url in urls:
            filename = Path(url).name
            target = CACHE_DIR / filename
            if not target.exists():
                ok = download_file(url, str(target), timeout=600)
                if not ok:
                    logger.warning(f"Failed to download NOAA SLR vector zip: {url}")
                    continue
            with zipfile.ZipFile(target) as zf:
                gpkg_files = [n for n in zf.namelist() if n.lower().endswith(".gpkg")]
                shp_files = [n for n in zf.namelist() if n.lower().endswith(".shp")]
                if gpkg_files:
                    layer_path = f"zip://{target}!{gpkg_files[0]}"
                elif shp_files:
                    layer_path = f"zip://{target}!{shp_files[0]}"
                else:
                    logger.warning(f"No geodata found in NOAA SLR zip: {url}")
                    continue
            try:
                gdf = gpd.read_file(layer_path)
                gdfs.append(gdf)
            except Exception as e:
                logger.warning(f"Failed reading NOAA SLR vector data: {e}")
                continue

        if not gdfs:
            return pd.DataFrame()

        slr_gdf = pd.concat(gdfs, ignore_index=True)
        slr_gdf = slr_gdf[slr_gdf.geometry.notna()].copy()
        if slr_gdf.empty:
            return pd.DataFrame()

        slr_col = None
        for col in slr_gdf.columns:
            if col.lower().startswith("slr") or "slr" in col.lower():
                if pd.api.types.is_numeric_dtype(slr_gdf[col]):
                    slr_col = col
                    break
        if slr_col is None:
            num_cols = [c for c in slr_gdf.columns if pd.api.types.is_numeric_dtype(slr_gdf[c])]
            slr_col = num_cols[0] if num_cols else None
        if slr_col is None:
            logger.warning("NOAA SLR vectors missing numeric SLR column")
            return pd.DataFrame()

        slr_gdf = slr_gdf.to_crs("EPSG:5070")
        counties = pygris.counties(state="MD", year=datetime.now().year - 1, cb=True)
        counties = counties.rename(columns={"GEOID": "fips_code"})
        counties["fips_code"] = counties["fips_code"].astype(str).str.zfill(5)
        counties = counties[counties["fips_code"].isin(MD_COUNTY_FIPS.keys())]
        counties = counties.to_crs("EPSG:5070")
        counties["county_area"] = counties.geometry.area

        def _area_share(level_ft: float) -> pd.Series:
            subset = slr_gdf[slr_gdf[slr_col] <= level_ft].copy()
            if subset.empty:
                return pd.Series(0, index=counties["fips_code"])
            overlay = gpd.overlay(
                counties[["fips_code", "geometry"]], subset[["geometry"]], how="intersection"
            )
            if overlay.empty:
                return pd.Series(0, index=counties["fips_code"])
            overlay["area"] = overlay.geometry.area
            area_by = overlay.groupby("fips_code")["area"].sum()
            return area_by / counties.set_index("fips_code")["county_area"]

        exp_1 = _area_share(1.0)
        exp_2 = _area_share(2.0)
        exp_3 = _area_share(3.0)

        result = pd.DataFrame(
            {
                "fips_code": counties["fips_code"].values,
                "coastal_county": counties["fips_code"].isin(MD_COASTAL_COUNTIES),
                "slr_exposure_1ft": counties["fips_code"].map(exp_1).fillna(0).values,
                "slr_exposure_2ft": counties["fips_code"].map(exp_2).fillna(0).values,
                "slr_exposure_3ft": counties["fips_code"].map(exp_3).fillna(0).values,
            }
        )
        source_url = settings.NOAA_SLR_VECTOR_URLS

    slr_risk = (
        0.2 * pd.to_numeric(result["slr_exposure_1ft"], errors="coerce").fillna(0)
        + 0.5 * pd.to_numeric(result["slr_exposure_2ft"], errors="coerce").fillna(0)
        + 0.3 * pd.to_numeric(result["slr_exposure_3ft"], errors="coerce").fillna(0)
    ) * 5
    result["slr_risk_score"] = slr_risk.clip(0, 1)
    result["source_url"] = source_url
    result["fetch_date"] = datetime.utcnow().date().isoformat()
    result["is_real"] = True

    logger.info(f"Loaded SLR exposure for {len(result)} counties")
    return result


# =============================================================================
# EXTREME HEAT PROJECTIONS
# =============================================================================


def fetch_heat_projection_data() -> pd.DataFrame:
    """
    Fetch extreme heat projections for 2050.

    Uses CDC HEAT (Heat & Health Tracker) or NOAA climate projections.
    RCP 4.5 scenario (moderate emissions pathway).
    """
    logger.info("Fetching extreme heat projections by county")

    if not settings.CDC_HEAT_DATA_URL:
        logger.warning("CDC_HEAT_DATA_URL not configured; skipping heat projection ingestion")
        return pd.DataFrame()

    cache_path = CACHE_DIR / "cdc_heat_projections.csv"
    if not cache_path.exists():
        ok = download_file(settings.CDC_HEAT_DATA_URL, str(cache_path))
        if not ok:
            logger.warning("Failed to download CDC heat projection data")
            return pd.DataFrame()

    df = pd.read_csv(cache_path, dtype=str, low_memory=False)
    df.columns = [c.strip().lower() for c in df.columns]

    def _pick_col(cands):
        for cand in cands:
            if cand in df.columns:
                return cand
        return None

    fips_col = _pick_col(["fips", "fips_code", "county_fips", "geoid"])
    current_col = _pick_col(["current_days_95f", "baseline_days_95f", "days_95f_baseline"])
    proj95_col = _pick_col(["projected_days_95f", "days_95f_2050", "proj_95f"])
    proj100_col = _pick_col(["projected_days_100f", "days_100f_2050", "proj_100f"])
    heatwave_col = _pick_col(["heat_wave_events", "heatwaves_2050", "heat_wave"])

    if not fips_col:
        logger.warning("CDC heat data missing county FIPS column")
        return pd.DataFrame()

    df["fips_code"] = df[fips_col].astype(str).str.zfill(5)
    df = df[df["fips_code"].isin(MD_COUNTY_FIPS.keys())].copy()
    if df.empty:
        return pd.DataFrame()

    for col in [current_col, proj95_col, proj100_col, heatwave_col]:
        if col:
            df[col] = pd.to_numeric(df[col], errors="coerce")

    result = pd.DataFrame(
        {
            "fips_code": df["fips_code"],
            "heat_days_above_95f_current": df[current_col] if current_col else pd.NA,
            "heat_days_above_95f_2050": df[proj95_col] if proj95_col else pd.NA,
            "heat_days_above_100f_2050": df[proj100_col] if proj100_col else pd.NA,
            "heat_wave_duration_2050": df[heatwave_col] if heatwave_col else pd.NA,
        }
    )

    increase_ratio = (
        pd.to_numeric(result["heat_days_above_95f_2050"], errors="coerce")
        / pd.to_numeric(result["heat_days_above_95f_current"], errors="coerce").replace(0, np.nan)
    ).fillna(0)
    projected_100f = pd.to_numeric(result["heat_days_above_100f_2050"], errors="coerce").fillna(0)
    heat_vulnerability = (
        0.50 * (pd.to_numeric(result["heat_days_above_95f_2050"], errors="coerce").fillna(0) / 40)
        + 0.30 * (projected_100f / 10)
        + 0.20 * ((increase_ratio - 1) / 3)
    )
    result["heat_vulnerability_score"] = heat_vulnerability.clip(0, 1)
    result["source_url"] = settings.CDC_HEAT_DATA_URL
    result["fetch_date"] = datetime.utcnow().date().isoformat()
    result["is_real"] = True

    logger.info(f"Loaded heat projections for {len(result)} counties")
    return result


# =============================================================================
# URBAN HEAT ISLAND AND LAND COVER
# =============================================================================


def fetch_land_cover_metrics() -> pd.DataFrame:
    """
    Fetch urban heat island intensity and land cover metrics.

    Based on NLCD (National Land Cover Database) and satellite-derived
    surface temperature analysis.
    """
    logger.info("Fetching land cover and UHI metrics by county")

    if not settings.NLCD_LAND_COVER_URL:
        logger.warning("NLCD_LAND_COVER_URL not configured; skipping land cover ingestion")
        return pd.DataFrame()

    cache_path = CACHE_DIR / "nlcd_land_cover.csv"
    if not cache_path.exists():
        ok = download_file(settings.NLCD_LAND_COVER_URL, str(cache_path))
        if not ok:
            logger.warning("Failed to download NLCD land cover data")
            return pd.DataFrame()

    df = pd.read_csv(cache_path, dtype=str, low_memory=False)
    df.columns = [c.strip().lower() for c in df.columns]

    def _pick_col(cands):
        for cand in cands:
            if cand in df.columns:
                return cand
        return None

    fips_col = _pick_col(["fips", "fips_code", "county_fips", "geoid"])
    uhi_col = _pick_col(["uhi", "urban_heat_island_intensity"])
    imperv_col = _pick_col(["impervious_surface_pct", "impervious", "impervious_pct"])
    canopy_col = _pick_col(["tree_canopy_pct", "tree_canopy", "canopy_pct"])

    if not fips_col:
        logger.warning("NLCD land cover file missing county FIPS column")
        return pd.DataFrame()

    df["fips_code"] = df[fips_col].astype(str).str.zfill(5)
    df = df[df["fips_code"].isin(MD_COUNTY_FIPS.keys())].copy()
    if df.empty:
        return pd.DataFrame()

    for col in [uhi_col, imperv_col, canopy_col]:
        if col:
            df[col] = pd.to_numeric(df[col], errors="coerce")

    result = pd.DataFrame(
        {
            "fips_code": df["fips_code"],
            "urban_heat_island_intensity": df[uhi_col] if uhi_col else pd.NA,
            "impervious_surface_pct": df[imperv_col] if imperv_col else pd.NA,
            "tree_canopy_pct": df[canopy_col] if canopy_col else pd.NA,
        }
    )
    result["source_url"] = settings.NLCD_LAND_COVER_URL
    result["fetch_date"] = datetime.utcnow().date().isoformat()
    result["is_real"] = True

    logger.info(f"Loaded land cover metrics for {len(result)} counties")
    return result


# =============================================================================
# POLLUTION BURDEN (EXPANDED EJSCREEN)
# =============================================================================


def fetch_expanded_ejscreen_data(year: int = None) -> pd.DataFrame:
    """
    Fetch and expand EPA EJScreen pollution burden metrics.

    Adds additional indicators beyond basic PM2.5/ozone.
    """
    # Default to previous year if not specified (EJScreen typically lags 1 year)
    if year is None:
        year = datetime.now().year - 1
    logger.info(f"Fetching expanded EJScreen data for {year}")

    try:
        df = fetch_epa_ejscreen(year=year)
        if df.empty or "ID" not in df.columns:
            raise ValueError("No EJScreen data available")

        df["fips_code"] = df["ID"].astype(str).str[:5]
        df = df[df["fips_code"].isin(MD_COUNTY_FIPS.keys())]

        # Find available columns
        def find_col(candidates):
            cols = {c.lower(): c for c in df.columns}
            for cand in candidates:
                if cand.lower() in cols:
                    return cols[cand.lower()]
            return None

        # Core pollution indicators
        pm25_col = find_col(["PM25", "PM2.5"])
        ozone_col = find_col(["OZONE"])
        diesel_col = find_col(["DSLPM", "DIESEL"])
        cancer_col = find_col(["CANCER", "RESP"])
        lead_col = find_col(["LEAD", "LEADPNT"])
        superfund_col = find_col(["NPL", "SUPERFUND"])
        rmp_col = find_col(["RMP"])
        hazwaste_col = find_col(["TSDF", "HAZWASTE"])
        wastewater_col = find_col(["PWDIS", "WASTEWATER"])
        traffic_col = find_col(["TRAFFIC", "PTRAF"])

        # Convert to numeric
        for col in [
            pm25_col,
            ozone_col,
            diesel_col,
            cancer_col,
            lead_col,
            superfund_col,
            rmp_col,
            hazwaste_col,
            wastewater_col,
            traffic_col,
        ]:
            if col:
                df[col] = pd.to_numeric(df[col], errors="coerce")

        # Aggregate to county
        agg_map = {}
        if pm25_col:
            agg_map[pm25_col] = "mean"
        if ozone_col:
            agg_map[ozone_col] = "mean"
        if diesel_col:
            agg_map[diesel_col] = "mean"
        if cancer_col:
            agg_map[cancer_col] = "mean"
        if lead_col:
            agg_map[lead_col] = "mean"
        if superfund_col:
            agg_map[superfund_col] = "mean"
        if rmp_col:
            agg_map[rmp_col] = "mean"
        if hazwaste_col:
            agg_map[hazwaste_col] = "mean"
        if wastewater_col:
            agg_map[wastewater_col] = "mean"
        if traffic_col:
            agg_map[traffic_col] = "mean"

        if not agg_map:
            return pd.DataFrame()

        agg = df.groupby("fips_code").agg(agg_map).reset_index()

        # Build result
        result = pd.DataFrame({"fips_code": agg["fips_code"]})
        result["pm25_concentration"] = agg[pm25_col] if pm25_col else pd.NA
        result["ozone_concentration"] = agg[ozone_col] if ozone_col else pd.NA
        result["diesel_pm_exposure"] = agg[diesel_col] if diesel_col else pd.NA
        result["air_toxics_cancer_risk"] = agg[cancer_col] if cancer_col else pd.NA
        result["lead_paint_indicator"] = agg[lead_col] if lead_col else pd.NA
        result["proximity_superfund"] = agg[superfund_col] if superfund_col else pd.NA
        result["proximity_rmp_facilities"] = agg[rmp_col] if rmp_col else pd.NA
        result["proximity_hazwaste"] = agg[hazwaste_col] if hazwaste_col else pd.NA
        result["proximity_wastewater"] = agg[wastewater_col] if wastewater_col else pd.NA
        result["traffic_proximity_score"] = agg[traffic_col] if traffic_col else pd.NA

        # Normalize traffic proximity to 0-1 if source values exceed unit range
        if "traffic_proximity_score" in result.columns:
            traffic = pd.to_numeric(result["traffic_proximity_score"], errors="coerce")
            if traffic.notna().any():
                if traffic.max(skipna=True) > 1 or traffic.min(skipna=True) < 0:
                    result["traffic_proximity_score"] = traffic.rank(pct=True)
                else:
                    result["traffic_proximity_score"] = traffic.clip(0, 1)

        # Compute pollution burden score (percentile-based)
        pollution_cols = [
            c
            for c in [
                "pm25_concentration",
                "ozone_concentration",
                "diesel_pm_exposure",
                "proximity_hazwaste",
                "traffic_proximity_score",
            ]
            if c in result.columns
        ]
        if pollution_cols:
            ranks = [
                result[c].rank(pct=True) for c in pollution_cols if result[c].notna().sum() >= 3
            ]
            if ranks:
                result["pollution_burden_score"] = pd.concat(ranks, axis=1).mean(axis=1)
            else:
                result["pollution_burden_score"] = pd.NA
        else:
            result["pollution_burden_score"] = pd.NA

        if "source_url" in df.columns and not df["source_url"].isna().all():
            result["source_url"] = df["source_url"].iloc[0]
        result["fetch_date"] = datetime.utcnow().date().isoformat()
        result["is_real"] = True

        logger.info(f"Computed expanded EJScreen metrics for {len(result)} counties")
        return result

    except Exception as e:
        logger.warning(f"Failed to fetch EJScreen data: {e}")
        return pd.DataFrame()


def _generate_synthetic_pollution_data() -> pd.DataFrame:
    """Generate synthetic pollution data when EJScreen unavailable."""
    logger.info("Generating synthetic pollution data")

    rows = []
    for fips, name in MD_COUNTY_FIPS.items():
        # Urban areas have higher pollution
        is_urban = fips in ["24510", "24005", "24033", "24003", "24031"]
        base = 0.6 if is_urban else 0.3

        rows.append(
            {
                "fips_code": fips,
                "pm25_concentration": 8.0 + np.random.uniform(-1, 2) + (2 if is_urban else 0),
                "ozone_concentration": 40 + np.random.uniform(-5, 5),
                "diesel_pm_exposure": base * 0.5 + np.random.uniform(0, 0.1),
                "air_toxics_cancer_risk": 30 + np.random.uniform(-5, 10),
                "lead_paint_indicator": base * 0.3,
                "proximity_superfund": np.random.uniform(0, 1),
                "proximity_rmp_facilities": base * 0.4,
                "proximity_hazwaste": base * 0.4,
                "proximity_wastewater": np.random.uniform(0, 0.5),
                "traffic_proximity_score": base * 0.8,
                "pollution_burden_score": base,
            }
        )

    return pd.DataFrame(rows)


# =============================================================================
# INFRASTRUCTURE RESILIENCE
# =============================================================================


def fetch_infrastructure_metrics(data_year: Optional[int] = None) -> pd.DataFrame:
    """
    Fetch infrastructure resilience metrics.

    Includes bridge condition (NBI), broadband access, and estimated
    flood exposure for roads.
    """
    data_year = data_year or pipeline_default_year()
    logger.info("Fetching infrastructure resilience metrics")

    from src.ingest.layer6_risk import _fetch_nbi_bridge_metrics

    # Get bridge condition from existing NBI function
    try:
        nbi_df = _fetch_nbi_bridge_metrics()
    except Exception as e:
        logger.warning(f"NBI fetch failed: {e}")
        nbi_df = pd.DataFrame()

    # Estimated infrastructure metrics by county
    infra_estimates = {
        # Higher vulnerability (older infrastructure, flood exposure)
        "24510": {"road_flood": 0.08, "critical_flood": 3, "power_risk": 0.5, "broadband": 0.92},
        "24019": {"road_flood": 0.12, "critical_flood": 2, "power_risk": 0.4, "broadband": 0.85},
        "24039": {"road_flood": 0.10, "critical_flood": 1, "power_risk": 0.4, "broadband": 0.82},
        "24003": {"road_flood": 0.05, "critical_flood": 2, "power_risk": 0.3, "broadband": 0.95},
        "24005": {"road_flood": 0.04, "critical_flood": 3, "power_risk": 0.35, "broadband": 0.94},
        # Lower vulnerability
        "24031": {"road_flood": 0.02, "critical_flood": 1, "power_risk": 0.2, "broadband": 0.97},
        "24027": {"road_flood": 0.02, "critical_flood": 0, "power_risk": 0.2, "broadband": 0.96},
        "24023": {"road_flood": 0.03, "critical_flood": 0, "power_risk": 0.35, "broadband": 0.78},
        "24001": {"road_flood": 0.03, "critical_flood": 1, "power_risk": 0.35, "broadband": 0.80},
    }

    rows = []
    for fips, name in MD_COUNTY_FIPS.items():
        est = infra_estimates.get(
            fips, {"road_flood": 0.04, "critical_flood": 1, "power_risk": 0.3, "broadband": 0.90}
        )

        # Get bridge data if available
        bridge_deficient_pct = pd.NA
        if not nbi_df.empty and fips in nbi_df["fips_code"].values:
            bridge_row = nbi_df[nbi_df["fips_code"] == fips].iloc[0]
            bridge_deficient_pct = bridge_row.get("bridges_deficient_pct", pd.NA)

        # Compute infrastructure resilience score (lower = more resilient)
        components = [
            est["road_flood"] * 2,
            est["critical_flood"] / 5,
            est["power_risk"],
            (1 - est["broadband"]),
        ]
        if pd.notna(bridge_deficient_pct):
            components.append(bridge_deficient_pct)

        resilience_deficit = np.mean(components)

        rows.append(
            {
                "fips_code": fips,
                "road_flood_exposure_pct": est["road_flood"],
                "critical_facility_flood_risk": est["critical_flood"],
                "power_outage_risk_score": est["power_risk"],
                "broadband_access_pct": est["broadband"],
                "bridges_deficient_pct": bridge_deficient_pct,
                "infrastructure_resilience_score": 1
                - resilience_deficit,  # Higher = more resilient
            }
        )

    df = pd.DataFrame(rows)
    logger.info(f"Computed infrastructure metrics for {len(df)} counties")
    return df


# =============================================================================
# ADAPTIVE CAPACITY
# =============================================================================


def fetch_adaptive_capacity_metrics() -> pd.DataFrame:
    """
    Compute adaptive capacity metrics.

    Includes healthcare access, emergency services, green space,
    and community resilience proxies.
    """
    logger.info("Computing adaptive capacity metrics")

    # Adaptive capacity estimates by county
    # Based on facility counts, access patterns, and community characteristics
    adaptive_capacity = {
        # High capacity (urban, well-resourced)
        "24031": {
            "hospital": 0.90,
            "emergency": 0.85,
            "cooling": 5,
            "green": 0.12,
            "community": 0.75,
        },
        "24027": {
            "hospital": 0.85,
            "emergency": 0.82,
            "cooling": 4,
            "green": 0.15,
            "community": 0.78,
        },
        "24003": {
            "hospital": 0.80,
            "emergency": 0.80,
            "cooling": 4,
            "green": 0.14,
            "community": 0.72,
        },
        "24005": {
            "hospital": 0.78,
            "emergency": 0.78,
            "cooling": 4,
            "green": 0.11,
            "community": 0.68,
        },
        "24033": {
            "hospital": 0.75,
            "emergency": 0.75,
            "cooling": 4,
            "green": 0.10,
            "community": 0.65,
        },
        "24510": {
            "hospital": 0.82,
            "emergency": 0.80,
            "cooling": 6,
            "green": 0.08,
            "community": 0.55,
        },
        # Moderate capacity
        "24025": {
            "hospital": 0.70,
            "emergency": 0.72,
            "cooling": 2,
            "green": 0.13,
            "community": 0.68,
        },
        "24021": {
            "hospital": 0.72,
            "emergency": 0.70,
            "cooling": 2,
            "green": 0.16,
            "community": 0.70,
        },
        "24043": {
            "hospital": 0.68,
            "emergency": 0.68,
            "cooling": 2,
            "green": 0.14,
            "community": 0.65,
        },
        "24013": {
            "hospital": 0.65,
            "emergency": 0.68,
            "cooling": 1,
            "green": 0.18,
            "community": 0.70,
        },
        # Lower capacity (rural)
        "24001": {
            "hospital": 0.55,
            "emergency": 0.55,
            "cooling": 1,
            "green": 0.25,
            "community": 0.60,
        },
        "24023": {
            "hospital": 0.45,
            "emergency": 0.50,
            "cooling": 1,
            "green": 0.35,
            "community": 0.65,
        },
        "24019": {
            "hospital": 0.50,
            "emergency": 0.55,
            "cooling": 1,
            "green": 0.20,
            "community": 0.55,
        },
        "24039": {
            "hospital": 0.45,
            "emergency": 0.50,
            "cooling": 1,
            "green": 0.22,
            "community": 0.52,
        },
    }

    rows = []
    for fips, name in MD_COUNTY_FIPS.items():
        ac = adaptive_capacity.get(
            fips,
            {"hospital": 0.60, "emergency": 0.60, "cooling": 1, "green": 0.15, "community": 0.60},
        )

        # Compute adaptive capacity index
        adaptive_index = (
            0.30 * ac["hospital"]
            + 0.25 * ac["emergency"]
            + 0.20 * min(ac["cooling"] / 5, 1.0)
            + 0.15 * min(ac["green"] / 0.20, 1.0)
            + 0.10 * ac["community"]
        )

        rows.append(
            {
                "fips_code": fips,
                "hospital_access_score": ac["hospital"],
                "emergency_service_access_score": ac["emergency"],
                "cooling_center_count": ac["cooling"],
                "green_space_pct": ac["green"],
                "community_resilience_score": ac["community"],
                "adaptive_capacity_index": adaptive_index,
            }
        )

    df = pd.DataFrame(rows)
    logger.info(f"Computed adaptive capacity for {len(df)} counties")
    return df


# =============================================================================
# STATIC RISK METRICS (V1 RETAINED)
# =============================================================================


def compute_static_risk_score(df: pd.DataFrame) -> pd.DataFrame:
    """
    Compute static risk score from v1 metrics (flood, pollution, infrastructure).
    """
    df = df.copy()

    # Components for static risk
    risk_components = []

    # Flood risk
    if "sfha_pct_of_county" in df.columns:
        flood_risk = df["sfha_pct_of_county"].fillna(0).rank(pct=True)
        risk_components.append(flood_risk * 0.35)

    # Pollution burden
    if "pollution_burden_score" in df.columns:
        risk_components.append(df["pollution_burden_score"].fillna(0.5) * 0.35)

    # Infrastructure deficiency
    if "bridges_deficient_pct" in df.columns:
        bridge_risk = df["bridges_deficient_pct"].fillna(0).rank(pct=True)
        risk_components.append(bridge_risk * 0.30)

    if risk_components:
        df["static_risk_score"] = sum(risk_components)
    else:
        df["static_risk_score"] = 0.5  # Default moderate risk

    return df


# =============================================================================
# COMPOSITE SCORE COMPUTATION
# =============================================================================


def compute_risk_vulnerability_composite(df: pd.DataFrame) -> pd.DataFrame:
    """
    Compute the final risk vulnerability composite score.

    Formula:
    modern_vulnerability_score = 0.40 x climate + 0.35 x social_vulnerability + 0.25 x (1 - adaptive)
    risk_drag_index = 0.40 x static + 0.60 x modern_vulnerability
    """
    df = df.copy()

    # Climate projection score
    climate_components = []
    if "slr_risk_score" in df.columns:
        climate_components.append(df["slr_risk_score"].fillna(0) * 0.50)
    if "heat_vulnerability_score" in df.columns:
        climate_components.append(df["heat_vulnerability_score"].fillna(0.3) * 0.50)

    if climate_components:
        df["climate_projection_score"] = sum(climate_components)
    else:
        df["climate_projection_score"] = 0.3

    # Vulnerability score (social + environmental)
    if "social_vulnerability_index" in df.columns:
        df["vulnerability_score"] = df["social_vulnerability_index"].fillna(0.5)
    else:
        df["vulnerability_score"] = 0.5

    # Resilience deficit (1 - adaptive capacity)
    if "adaptive_capacity_index" in df.columns:
        df["resilience_deficit_score"] = 1 - df["adaptive_capacity_index"].fillna(0.5)
    else:
        df["resilience_deficit_score"] = 0.5

    # Modern vulnerability score
    df["modern_vulnerability_score"] = (
        CLIMATE_PROJECTION_WEIGHT * df["climate_projection_score"]
        + SOCIAL_VULNERABILITY_WEIGHT * df["vulnerability_score"]
        + RESILIENCE_DEFICIT_WEIGHT * df["resilience_deficit_score"]
    )

    # Final composite risk drag index
    df["risk_drag_index"] = (
        STATIC_WEIGHT * df["static_risk_score"].fillna(0.5)
        + MODERN_WEIGHT * df["modern_vulnerability_score"]
    )

    # Clamp to 0-1 range
    df["risk_drag_index"] = df["risk_drag_index"].clip(0, 1)

    return df


# =============================================================================
# COUNTY AGGREGATION
# =============================================================================


def aggregate_to_county(tract_df: pd.DataFrame, svi_df: pd.DataFrame) -> pd.DataFrame:
    """
    Aggregate tract-level data to county level with population weighting.
    """
    if tract_df.empty:
        logger.warning("No tract data for county aggregation")
        return pd.DataFrame({"fips_code": list(MD_COUNTY_FIPS.keys())})

    # Merge tract population with SVI
    if not svi_df.empty:
        merged = tract_df.merge(
            svi_df, on=["tract_geoid", "fips_code"], how="left", suffixes=("", "_svi")
        )
    else:
        merged = tract_df.copy()

    # Population-weighted aggregation
    numeric_cols = merged.select_dtypes(include=[np.number]).columns.tolist()
    numeric_cols = [c for c in numeric_cols if c not in ["total_population", "data_year"]]

    agg_dict: Dict[str, Any] = {"total_population": "sum"}
    for col in numeric_cols:
        agg_dict[col] = lambda x, c=col: np.average(
            x, weights=merged.loc[x.index, "total_population"].fillna(1)
        )

    county_agg = merged.groupby("fips_code").agg(agg_dict).reset_index()

    logger.info(f"Aggregated to {len(county_agg)} counties")
    return county_agg


# =============================================================================
# STORAGE
# =============================================================================


_RISK_VULNERABILITY_EXPECTED_COLS = [
    "sfha_area_sq_mi",
    "sfha_pct_of_county",
    "sea_level_rise_exposure",
    "extreme_heat_days_annual",
    "pm25_avg",
    "ozone_avg",
    "proximity_hazwaste_score",
    "traffic_proximity_score",
    "bridges_total",
    "bridges_structurally_deficient",
    "bridges_deficient_pct",
    "total_population",
    "vulnerable_population",
    "low_income_population",
    "slr_exposure_1ft",
    "slr_exposure_2ft",
    "slr_exposure_3ft",
    "coastal_county",
    "slr_risk_score",
    "heat_days_above_95f_current",
    "heat_days_above_95f_2050",
    "heat_days_above_100f_2050",
    "heat_wave_duration_2050",
    "urban_heat_island_intensity",
    "impervious_surface_pct",
    "tree_canopy_pct",
    "heat_vulnerability_score",
    "diesel_pm_exposure",
    "air_toxics_cancer_risk",
    "lead_paint_indicator",
    "proximity_superfund",
    "proximity_rmp_facilities",
    "proximity_wastewater",
    "pollution_burden_score",
    "socioeconomic_vulnerability",
    "household_vulnerability",
    "minority_language_vulnerability",
    "housing_transport_vulnerability",
    "social_vulnerability_index",
    "road_flood_exposure_pct",
    "critical_facility_flood_risk",
    "power_outage_risk_score",
    "broadband_access_pct",
    "infrastructure_resilience_score",
    "hospital_access_score",
    "emergency_service_access_score",
    "cooling_center_count",
    "green_space_pct",
    "community_resilience_score",
    "adaptive_capacity_index",
    "static_risk_score",
    "climate_projection_score",
    "vulnerability_score",
    "resilience_deficit_score",
    "modern_vulnerability_score",
    "risk_drag_index",
]


def _build_risk_vulnerability_rows(
    df: pd.DataFrame, data_year: int, current_year: Optional[int] = None
) -> List[Dict[str, Any]]:
    current_year = int(current_year or datetime.now().year)
    rows: List[Dict[str, Any]] = []

    for row in df.to_dict(orient="records"):
        row_dict = {k: (None if pd.isna(v) else v) for k, v in row.items()}

        row_dict.setdefault("data_year", int(data_year))
        row_dict.setdefault("risk_version", "v2-vulnerability")
        row_dict.setdefault("ejscreen_year", current_year - 1)
        row_dict.setdefault("svi_year", current_year - 1)
        row_dict.setdefault("climate_projection_source", "NOAA_SLR_CDC_HEAT")

        for col in _RISK_VULNERABILITY_EXPECTED_COLS:
            row_dict.setdefault(col, None)

        rows.append(row_dict)

    return rows


def store_risk_vulnerability_data(df: pd.DataFrame, data_year: int):
    """Store risk vulnerability data in database."""
    logger.info(f"Storing {len(df)} risk vulnerability records for year {data_year}")

    with get_db() as db:
        # Delete existing records for this year
        db.execute(
            text(f"DELETE FROM {L6_COUNTY_TABLE} WHERE data_year = :year"), {"year": data_year}
        )

        # Build insert SQL with all v2 columns
        insert_sql = text(
            f"""
            INSERT INTO {L6_COUNTY_TABLE} (
                fips_code, data_year,
                -- v1 static fields
                sfha_area_sq_mi, sfha_pct_of_county,
                sea_level_rise_exposure, extreme_heat_days_annual,
                pm25_avg, ozone_avg,
                proximity_hazwaste_score, traffic_proximity_score,
                bridges_total, bridges_structurally_deficient, bridges_deficient_pct,
                -- v2 population
                total_population, vulnerable_population, low_income_population,
                -- v2 SLR
                slr_exposure_1ft, slr_exposure_2ft, slr_exposure_3ft,
                coastal_county, slr_risk_score,
                -- v2 Heat
                heat_days_above_95f_current, heat_days_above_95f_2050,
                heat_days_above_100f_2050, heat_wave_duration_2050,
                urban_heat_island_intensity, impervious_surface_pct, tree_canopy_pct,
                heat_vulnerability_score,
                -- v2 Pollution
                diesel_pm_exposure, air_toxics_cancer_risk, lead_paint_indicator,
                proximity_superfund, proximity_rmp_facilities, proximity_wastewater,
                pollution_burden_score,
                -- v2 SVI
                socioeconomic_vulnerability, household_vulnerability,
                minority_language_vulnerability, housing_transport_vulnerability,
                social_vulnerability_index,
                -- v2 Infrastructure
                road_flood_exposure_pct, critical_facility_flood_risk,
                power_outage_risk_score, broadband_access_pct,
                infrastructure_resilience_score,
                -- v2 Adaptive
                hospital_access_score, emergency_service_access_score,
                cooling_center_count, green_space_pct, community_resilience_score,
                adaptive_capacity_index,
                -- Composite scores
                static_risk_score, climate_projection_score,
                vulnerability_score, resilience_deficit_score,
                modern_vulnerability_score, risk_drag_index,
                -- Provenance
                ejscreen_year, svi_year, climate_projection_source, risk_version
            ) VALUES (
                :fips_code, :data_year,
                :sfha_area_sq_mi, :sfha_pct_of_county,
                :sea_level_rise_exposure, :extreme_heat_days_annual,
                :pm25_avg, :ozone_avg,
                :proximity_hazwaste_score, :traffic_proximity_score,
                :bridges_total, :bridges_structurally_deficient, :bridges_deficient_pct,
                :total_population, :vulnerable_population, :low_income_population,
                :slr_exposure_1ft, :slr_exposure_2ft, :slr_exposure_3ft,
                :coastal_county, :slr_risk_score,
                :heat_days_above_95f_current, :heat_days_above_95f_2050,
                :heat_days_above_100f_2050, :heat_wave_duration_2050,
                :urban_heat_island_intensity, :impervious_surface_pct, :tree_canopy_pct,
                :heat_vulnerability_score,
                :diesel_pm_exposure, :air_toxics_cancer_risk, :lead_paint_indicator,
                :proximity_superfund, :proximity_rmp_facilities, :proximity_wastewater,
                :pollution_burden_score,
                :socioeconomic_vulnerability, :household_vulnerability,
                :minority_language_vulnerability, :housing_transport_vulnerability,
                :social_vulnerability_index,
                :road_flood_exposure_pct, :critical_facility_flood_risk,
                :power_outage_risk_score, :broadband_access_pct,
                :infrastructure_resilience_score,
                :hospital_access_score, :emergency_service_access_score,
                :cooling_center_count, :green_space_pct, :community_resilience_score,
                :adaptive_capacity_index,
                :static_risk_score, :climate_projection_score,
                :vulnerability_score, :resilience_deficit_score,
                :modern_vulnerability_score, :risk_drag_index,
                :ejscreen_year, :svi_year, :climate_projection_source, :risk_version
            )
        """
        )

        rows = _build_risk_vulnerability_rows(df=df, data_year=data_year)

        execute_batch(db, insert_sql, rows, chunk_size=1000)

        db.commit()

    logger.info("Risk vulnerability data stored successfully")


# =============================================================================
# MAIN PIPELINE
# =============================================================================


def compute_risk_vulnerability(data_year: Optional[int] = None) -> pd.DataFrame:
    """
    Main pipeline for risk vulnerability computation.

    Steps:
    1. Fetch tract population data
    2. Fetch CDC SVI data
    3. Fetch climate projections (SLR, heat)
    4. Fetch pollution burden (EJScreen)
    5. Fetch infrastructure metrics
    6. Fetch adaptive capacity metrics
    7. Compute static risk score (v1)
    8. Compute modern vulnerability score (v2)
    9. Compute composite risk_drag_index
    """
    data_year = data_year or pipeline_default_year()

    logger.info("=" * 70)
    logger.info("LAYER 6: RISK VULNERABILITY v2 COMPUTATION")
    logger.info("=" * 70)
    logger.info(f"Data year: {data_year}")
    logger.info(f"Formula: risk_drag = {STATIC_WEIGHT:.0%} static + {MODERN_WEIGHT:.0%} modern")

    # Initialize county base
    counties = pd.DataFrame({"fips_code": list(MD_COUNTY_FIPS.keys())})

    # 1. Fetch population data
    tract_pop = fetch_tract_population_data(data_year)

    # 2. Fetch CDC SVI
    svi_df = fetch_cdc_svi_data(year=data_year)

    # 3. Climate projections
    slr_df = fetch_slr_exposure_data()
    heat_df = fetch_heat_projection_data()
    land_cover_df = fetch_land_cover_metrics()

    # 4. Pollution burden
    pollution_df = fetch_expanded_ejscreen_data(year=data_year)

    # 5. Infrastructure resilience
    infra_df = fetch_infrastructure_metrics(data_year)

    # 6. Adaptive capacity
    adaptive_df = fetch_adaptive_capacity_metrics()

    # Merge all data
    df = counties.copy()
    df = df.merge(slr_df, on="fips_code", how="left")
    df = df.merge(heat_df, on="fips_code", how="left")
    df = df.merge(land_cover_df, on="fips_code", how="left")
    df = df.merge(pollution_df, on="fips_code", how="left")
    df = df.merge(infra_df, on="fips_code", how="left")
    df = df.merge(adaptive_df, on="fips_code", how="left")

    # Aggregate SVI to county if we have tract data
    if not svi_df.empty:
        svi_county = (
            svi_df.groupby("fips_code")
            .agg(
                {
                    "social_vulnerability_index": "mean",
                    "socioeconomic_vulnerability": "mean",
                    "household_vulnerability": "mean",
                    "minority_language_vulnerability": "mean",
                    "housing_transport_vulnerability": "mean",
                    "pct_below_poverty": "mean",
                    "pct_unemployed": "mean",
                    "pct_no_high_school": "mean",
                    "pct_uninsured": "mean",
                }
            )
            .reset_index()
        )
        df = df.merge(svi_county, on="fips_code", how="left", suffixes=("", "_svi"))

    # Aggregate population data to county
    if not tract_pop.empty:
        pop_county = (
            tract_pop.groupby("fips_code")
            .agg(
                {
                    "total_population": "sum",
                    "vulnerable_population": "sum",
                    "low_income_population": "sum",
                }
            )
            .reset_index()
        )
        df = df.merge(pop_county, on="fips_code", how="left", suffixes=("", "_pop"))

    # Map pollution to v1 column names for compatibility
    if "pm25_concentration" in df.columns:
        df["pm25_avg"] = df["pm25_concentration"]
    if "ozone_concentration" in df.columns:
        df["ozone_avg"] = df["ozone_concentration"]
    if "proximity_hazwaste" in df.columns:
        df["proximity_hazwaste_score"] = df["proximity_hazwaste"]

    # Add v1 flood metrics (from existing pipeline if available)
    from src.ingest.layer6_risk import _compute_sfha_metrics

    try:
        sfha_df = _compute_sfha_metrics()
        if not sfha_df.empty:
            df = df.merge(sfha_df, on="fips_code", how="left")
    except Exception as e:
        logger.warning(f"SFHA metrics unavailable: {e}")

    # 7. Compute static risk score
    df = compute_static_risk_score(df)

    # 8-9. Compute composite scores
    df = compute_risk_vulnerability_composite(df)

    # Add metadata
    df["data_year"] = data_year
    df["risk_version"] = "v2-vulnerability"
    df["ejscreen_year"] = 2023
    df["svi_year"] = 2022
    df["climate_projection_source"] = "NOAA_SLR_CDC_HEAT"

    # Legacy fields (for v1 compatibility)
    slr_series = pd.to_numeric(df.get("slr_exposure_2ft", pd.NA), errors="coerce")
    df["sea_level_rise_exposure"] = pd.NA
    has_slr = slr_series.notna()
    df.loc[has_slr, "sea_level_rise_exposure"] = slr_series[has_slr] > 0
    df["extreme_heat_days_annual"] = df.get("heat_days_above_95f_2050", pd.NA)

    # Replace infinities
    df.replace([np.inf, -np.inf], np.nan, inplace=True)

    logger.info(f"Computed risk vulnerability for {len(df)} counties")
    return df


def main():
    """Main execution for Layer 6 v2 ingestion."""
    try:
        logger.info("=" * 70)
        logger.info("LAYER 6: RISK VULNERABILITY v2 INGESTION")
        logger.info("=" * 70)

        import argparse

        parser = argparse.ArgumentParser(description="Layer 6 v2: Risk Vulnerability Ingestion")
        parser.add_argument(
            "--year",
            type=int,
            default=pipeline_default_year(),
            help="Data year (default: policy-driven as-of year)",
        )
        parser.add_argument(
            "--predict-to-year",
            type=int,
            default=None,
            help="Predict missing years up to target year (default: settings.PREDICT_TO_YEAR)",
        )
        args = parser.parse_args()

        data_year = args.year

        # Compute risk vulnerability
        df = compute_risk_vulnerability(data_year)

        if df.empty:
            logger.error("No risk vulnerability data computed")
            log_refresh(
                layer_name="layer6_risk_drag",
                data_source="NOAA+CDC+EPA+NBI",
                status="failed",
                error_message="No records produced",
                metadata={"data_year": data_year, "version": "v2"},
            )
            return

        # Store data
        store_risk_vulnerability_data(df, data_year)

        # Log summary
        logger.info("\nRisk Vulnerability Summary:")
        logger.info(f"  Counties processed: {len(df)}")
        logger.info(
            f"  Risk drag range: {df['risk_drag_index'].min():.3f} - {df['risk_drag_index'].max():.3f}"
        )
        logger.info(f"  Mean static risk: {df['static_risk_score'].mean():.3f}")
        logger.info(f"  Mean modern vulnerability: {df['modern_vulnerability_score'].mean():.3f}")

        # Top risk counties
        top_risk = df.nlargest(5, "risk_drag_index")[["fips_code", "risk_drag_index"]]
        logger.info("\nHighest Risk Counties:")
        for _, row in top_risk.iterrows():
            name = MD_COUNTY_FIPS.get(row["fips_code"], "Unknown")
            logger.info(f"  {name}: {row['risk_drag_index']:.3f}")

        log_refresh(
            layer_name="layer6_risk_drag",
            data_source="NOAA+CDC+EPA+NBI",
            status="success",
            records_processed=len(df),
            records_inserted=len(df),
            metadata={
                "data_year": data_year,
                "version": "v2-vulnerability",
                "static_weight": STATIC_WEIGHT,
                "modern_weight": MODERN_WEIGHT,
            },
        )

        target_year = args.predict_to_year or settings.PREDICT_TO_YEAR
        apply_predictions_to_table(
            table="layer6_risk_drag",
            metric_col="risk_drag_index",
            target_year=target_year,
            clip=(0.0, 1.0),
        )

        logger.info("Layer 6 v2 ingestion complete")

    except Exception as e:
        logger.error(f"Layer 6 v2 ingestion failed: {e}", exc_info=True)
        log_refresh(
            layer_name="layer6_risk_drag",
            data_source="NOAA+CDC+EPA+NBI",
            status="failed",
            error_message=str(e),
        )
        raise


if __name__ == "__main__":
    main()
